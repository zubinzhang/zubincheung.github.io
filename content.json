{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"关于","text":"这是什么记录学习生活点滴，把自一些工作中学到的知识及解决问题的思路,整理记录下来,方便自己日后工作用到的时候能够翻翻，文章质量不要太期待。 关于我一个喜欢摄影的码畜，目前在 Node.JS 的道路上，正在学习 Go 和 React，希望成为一个全栈工程师。 Email：zubincheung@gmail.com Github：http://github.com/zubinzhang","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Node事件循环","text":"之前对 Node.js 总是有点似懂非懂的感觉，查阅一下官方文档https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/ 有一种恍然大悟的感觉 当 Node.js 启动时会初始化 event loop, 每一个 event loop 都会包含按如下顺序六个循环阶段： 123456789101112131415161718 ┌───────────────────────────┐┌─&gt;│ timers ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐│ │ pending callbacks ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐│ │ idle, prepare ││ └─────────────┬─────────────┘ ┌───────────────┐│ ┌─────────────┴─────────────┐ │ incoming: ││ │ poll │&lt;─────┤ connections, ││ └─────────────┬─────────────┘ │ data, etc. ││ ┌─────────────┴─────────────┐ └───────────────┘│ │ check ││ └─────────────┬─────────────┘│ ┌─────────────┴─────────────┐└──┤ close callbacks │ └───────────────────────────┘ timers 阶段: 这个阶段执行 setTimeout(callback) and setInterval(callback)预定的 callback; I/O callbacks 阶段: 执行除了 close 事件的 callbacks、被 timers(定时器，setTimeout、setInterval 等)设定的 callbacks、setImmediate()设定的 callbacks 之外的 callbacks; idle, prepare 阶段: 仅 node 内部使用; poll 阶段: 获取新的 I/O 事件, 适当的条件下 node 将阻塞在这里; check 阶段: 执行 setImmediate() 设定的 callbacks; close callbacks 阶段: 比如 socket.on(‘close’, callback)的 callback 会在这个阶段执行. 每一个阶段都有一个装有 callbacks 的 fifo queue(队列)，当 event loop 运行到一个指定阶段时，node 将执行该阶段的 fifo queue(队列)，当队列 callback 执行完或者执行 callbacks 数量超过该阶段的上限时，event loop 会转入下一下阶段. 注意上面六个阶段都不包括 process.nextTick() 了解了这些就不难理解以下代码的执行顺序 123456789101112131415setTimeout(function() { console.log('timeout1');});new Promise(function(resolve) { console.log('promise1'); for (var i = 0; i &lt; 1000; i++) { i == 99 &amp;&amp; resolve(); } console.log('promise2');}).then(function() { console.log('then1');});console.log('global1');","link":"/blog/b1812053.html"},{"title":"golang学习笔记","text":"golang 学习笔记整理 数组 go 语言中数组是值类型，这意味着当数组赋值给一个新的变量时，该变量会得到一个原始数组的一个副本。如果对新变量进行更改，则不会影响原始数组。 当数组作为参数传递给函数时，它们是按值传递，而原始数组保持不变。 引用类型在 golang 中只有三种引用类型它们分别是切片 slice、字典 map、管道 channel。其它的全部是值类型，引用类型可以简单的理解为指针类型，它们都是通过 make 完成初始化 runeRune 是 int32 的别名。用 UTF-8 进行编码。这个类型在什么时候使用呢？例如需要遍历字符串中的字符。可以循环每个字节（仅在使用 US ASCII 编码字符串时与字符等价，而它们在 Go 中不存在！）。因此为了获得实际的字符，需要使用 rune 类型。 在 UTF-8 世界的字符有时被称作 runes。通常，当人们讨论字符时，多数是指 8 位字符。UTF-8 字符可能会有 32 位，称作 rune 指针Go 有指针，但是没有指针运算。不能用指针变量遍历字符串的各个字节。因此它们更象是引用而不是你所知道的来自于 C 的指针。指针非常有用。 内存分配Go 有两个内存分配原语，new 和 make。它们应用于不同的类型，做不同的工作，可能有些迷惑人，但是规则很简单。 用 new 分配内存内建函数 new 本质上说跟其他语言中的同名函数功能一样：new(T)分配了零值填充的 T 类型的内存空间，并且返回其地址，一个*T 类型的值。用 Go 的术语说，它返回了一个指针，指向新分配的类型的零值。有一点非常重要：new 返回指针。 这意味着使用者可以用 new 创建一个数据结构的实例并且可以直接工作。 用 make 分配内存回到内存分配。内建函数 make(T,args)与 new(T)有着不同的功能。它只能创建 slice，map 和 channel，并且返回一个有初始值（非零）的 T 类型，而不是*T。本质来讲，导致这三个类型有所不同的原因是指向数据结构的引用在使用前必须被初始化。 make 返回初始化后的（非零）值。 务必记得 make 仅适用于 map，slice 和 channel，并且返回的不是指针。应当用 new 获得特定的指针。 new(T)返回*T 指向一个零值 T make(T)返回初始化后的 T 转换有时需要将一个类型转换为另一个类型。在 Go 中可以做到，不过有一些规则。首先，将一个值转换为另一个是由操作符（看起来像函数：byte()）完成的，并且不是所有的转换都是允许的。 定义自己的类型Go 允许定义新的类型，通过保留字 type 实现 1type foo int 创建了一个新的类型 foo 作用跟 int 一样。创建更加复杂的类型需要用到 struct 保留字。","link":"/blog/f20814c0.html"},{"title":"mac下go开发环境配置","text":"brew 安装 go123brew install go# 或者brew install golang 运行 go version,返回 go 版本即安装成功 此外还可通过源代码安装https://golang.google.cn/dl/ 设置 path这里用的是 zsh,编辑~/.zshrc,粘贴命令 1234# goexport GOPATH=$HOME/workspace/goexport GOBIN=$GOPATH/binexport PATH=$PATH:$GOBIN Delve 调试 Go 项目第一步：创建证书 打开钥匙串访问； 菜单栏中选择钥匙串访问-证书助理-创建证书开始创建自签名证书； 证书名称设置为 dlv-cert（记住这个名字，后面会用到）；身份类型选择自签名根证书；证书类型选择代码签名，最后在让我覆盖这些默认值处打上勾，选择继续； 在接下来的窗口中把有效期改长一些，例如改成 10 年（3650 天）； 然后一直往后，直到出现选择指定用于该证书的位置，选择钥匙串系统，然后选择创建； 这样证书就创建好了。 可能遇到的问题：创建证书时报未知错误解决方案：指定用于该证书的位置时先选择登录，创建成功后，这个证书显示在登录选项里面。选中这个证书直接拉到系统选项上，然后系统选项里也有了这个证书，最后删除登录里面的这个证书 第二步 ：信任证书 在钥匙串访问窗口左面选择钥匙串系统；然后在右面选择刚才创建的证书（按名字查找，例如 dlv-cert）； 点鼠标右键，选择显示简介打开证书详细信息窗口； 在信任一栏中代码签名处选择始终信任，这样使用该证书进行签名操作的时候就不会弹出提示框询问了。 然后在窗口左面选择密钥，在右面根据名字选择对应的专用密钥，点击鼠标右键选择显示简介； 在弹出的窗口中选择访问控制标签页，然后选择允许所有应用程序访问此项目，这样进行调试的时候就不需要每次输入密码了。 后续操作123go get -v -u github.com/peterh/liner github.com/derekparker/delve/cmd/dlvbrew install go-delve/delve/delvego get -v -u github.com/peterh/liner github.com/derekparker/delve/cmd/dlv 输入 dlv version 如果看到**Delve Debugger**的版本信息，则表明 Delve 安装成功了！ 使用 vscode 调试扩展：安装 go 插件点击右边的 Extensions 图标，搜索 Go 插件，选择 Go 进行安装，安装成功后重启 vscode 遇到的坑：按 f5 调试报错：could not launch process: EOF解决方案：https://github.com/derekparker/delve/issues/1165 12345678I rollback my CommandLineTools,Temporarily solved the problem by removing new Command Line Tools$ sudo rm -rf /Library/Developer/CommandLineToolsthen go to page https://developer.apple.com/download/more/download and install previous versionCommand Line Tools (macOS 10.13) for Xcode 9.2 - Dec 4, 2017","link":"/blog/3c39bb5f.html"},{"title":"《你不知道的JavaScript》笔记-this用法","text":"this 绑定规则this 是在运行时进行绑定的，并不是在编写时绑定，它的上下文取决于函数调用时的各种条件。this 的绑定和函数声明的位置没有任何关系，只取决于函数的调用方式。 当一个函数被调用时，会创建一个活动记录（有时候也称为执行上下文）。这个记录会包含函数在哪里被调用（调用栈）、函数的调用方式、传入的参数等信息。this 就是这个记录的一个属性，会在函数执行的过程中用到。 优先级：显式绑定&gt;隐式绑定 默认绑定函数独立调用时，非严格模式时函数调用时应用了 this 的默认绑定，this 指向全局对象 1234567function foo() { console.log(this.a);}var a = 2;foo(); // 2 如果使用严格模式（strict mode），则不能将全局对象用于默认绑定，因此 this 会绑定到 undefined： 123456789function foo() { 'use strict'; console.log(this.a);}var a = 2;foo(); // TypeError: this is undefined 隐式绑定当函数引用有上下文对象时，隐式绑定规则会把函数调用中的 this 绑定到这个上下文对象。对象属性引用链中只有上一层或者说最后一层在调用位置中起作用。 123456789101112function foo() { console.log(this.a);}var obj2 = { a: 42, foo: foo,};var obj1 = { a: 2, obj2: obj2,};obj1.obj2.foo(); // 42 隐式丢失： 被隐式绑定的函数会丢失绑定对象,它会应用默认绑定 调用回调函数的函数可能会修改 this。 1234567891011121314151617function foo() { console.log( this.a );}var obj = { a: 2, foo: foo};var bar = obj.foo; // 函数别名！￼var a = &quot;oops, global&quot;; // a是全局对象的属性// 应用默认绑定，把this绑定到全局对象或者undefined上，取决于是否严格模式。bar(); // &quot;oops, global&quot;setTimeout( obj.foo, 100 ); // &quot;oops, global&quot; More info: Server 显式绑定可以使用函数的 call(..)和 apply(..)方法，直接指定 this 的绑定对象,如果你把 null 或者 undefined 作为 this 的绑定对象传入 call、apply 或者 bind，这些值在调用时会被忽略，实际应用的是默认绑无论之后如何调用函数 bar，它总会手动在 obj 上调用 foo。这种绑定是一种显式的强制绑定，因此我们称之为硬绑定。 123456789101112function foo() { console.log(this.a);}var obj = { a: 2,};foo.call(obj); // 2// 硬绑定的bar不可能再修改它的thisbar.call(window); // 2 More info: Generating new 绑定使用 new 来调用函数，或者说发生构造函数调用时，会自动执行下面的操作。 创建（或者说构造）一个全新的对象。 这个新对象会被执行[[Prototype]]连接。 这个新对象会绑定到函数调用的 this。 如果函数没有返回其他对象，那么 new 表达式中的函数调用会自动返回这个新对象 使用 new 来调用 foo(..)时，我们会构造一个新对象并把它绑定到 foo(..)调用中的 this 上。new 是最后一种可以影响函数调用时 this 绑定行为的方法，我们称之为 new 绑定。 1234567function foo(a) { this.a = a;}var bar = new foo(2);console.log(bar.a); // 2 1234567891011function foo(p1, p2) { this.val = p1 + p2;}// 之所以使用null是因为在本例中我们并不关心硬绑定的this是什么// 反正使用new时this会被修改var bar = foo.bind(null, 'p1');var baz = new bar('p2');baz.val; // p1p2 判断 this 函数是否在 new 中调用（new 绑定）？如果是的话 this 绑定的是新创建的对象。 1var bar = new foo(); 函数是否通过 call、apply（显式绑定）或者硬绑定调用？如果是的话，this 绑定的是指定的对象。 1var bar = foo.call(obj2); 如果你把 null 或者 undefined 作为 this 的绑定对象传入 call、apply 或者 bind，这些值在调用时会被忽略，实际应用的是默认绑定规则： 1234567function foo() { console.log(this.a);}var a = 2;foo.call(null); // 2 函数是否在某个上下文对象中调用（隐式绑定）？如果是的话，this 绑定的是那个上下文对象。 1var bar = obj1.foo(); 如果都不是的话，使用默认绑定。如果在严格模式下，就绑定到 undefined，否则绑定到全局对象。 1var bar = foo(); 箭头函数不使用 this 的四种标准规则，而是根据外层（函数或者全局）作用域来决定 this 123456789101112131415161718function foo() { // 返回一个箭头函数 return a =&gt; { //this继承自foo() console.log(this.a); };}var obj1 = { a: 2,};var obj2 = { a: 3,};var bar = foo.call(obj1);bar.call(obj2); // 2, 不是3！","link":"/blog/6c835f69.html"},{"title":"《你不知道的JavaScript》笔记-作用域","text":"作用域是什么作用域是一套规则，用于确定在何处以及如何查找变量（标识符）。 编译原理当我们看到 var a=2; 的时候引擎和编译器会做什么呢? 遇到 var a,编译器会询问作用域是否已经有一个该名称的变量存在于同一个作用域的合集中.如果是.编译器会忽略该声明,继续进行编译.否则它会要求作用域在当前的作用域合集中声明一个新的变量,并且命名为 a. 接下来编译器会为这个引擎生成运行时所需的代码,这些代码被用来处理 a = 2 这个赋值操作.引擎运行时会首先询问作用域,在当前的作用域合集中是否存在一个叫 a 的变量,如果否,引擎就会使用这个变量;如果不是,引擎就会继续查找该变量. 理解作用域RHS 引用是找到这个变量所在的地址,但是不赋值 赋值是等号做的事情 LHS 引用是赋值时把 RHS 找到的地址赋值给 LHS如果查找的目的是对变量进行赋值，那么就会使用 LHS 查询；如果目的是获取变量的值，就会使用 RHS 查询。 作用域嵌套当一个块或函数嵌套在另一个块或函数中时，就发生了作用域的 嵌套。因此，在当前作用域中无法找到某个变量时，引擎就会在外层嵌套的作用域中继续查找，直到找到该变量，或抵达最外层的作用域（也就是全局作用域）为止。 异常不成功的 RHS 引用会导致抛出 ReferenceError 异常。不成功的 LHS 引用会导致自动隐式地创建一个全局变量（非严格模式下），该变量使用 LHS 引用的目标作为标识符，或者抛出 ReferenceError 异常（严格模式下）。 词法作用域词法阶段词法作用域也叫静态作用域.其作用域只在引擎初始化的时候就已经定好了.不会跟随代码的执行而动态改变作用域 1234567891011function foo(a) { var b = a * 2; function bar(c) { console.log(a, b, c); } bar(b * 3);}foo(2); // 2, 4, 12 这里面有三个嵌套的作用域 这里来分析一下： window(全局作用域) window=&gt;foo window=&gt;foo=&gt;bar 词法作用域意味着作用域是由书写代码时函数声明的位置来决定的。编译的词法分析阶段基本能够知道全部标识符在哪里以及是如何声明的，从而能够预测在执行过程中如何对它们进行查找。 欺骗词法JavaScript 中有两个机制可以 “欺骗” 词法作用域：eval(..)和 with。前者可以对一段包含一个或多个声明的“代码”字符串进行演算，并借此来修改已经存在的词法作用域（在运行时）。后者本质上是通过将一个对象的引用当作作用域来处理，将对象的属性当作作用域中的标识符来处理，从而创建了一个新的词法作用域（同样是在运行时）。 这两个机制的副作用是引擎无法在编译时对作用域查找进行优化，欺骗词法作用域会导致性能下降。不要使用它们。 在严格模式下 with 被完全禁止，eval 所生成的变量只能用于 eval 内部。 函数作用域和块作用域函数中的作用域函数是 JavaScript 中最常见的作用域单元。本质上，声明在一个函数内部的变量或函数会在所处的作用域中“隐藏”起来，这是有意为之的良好软件的设计原则。 但函数不是唯一的作用域单元。块作用域指的是变量和函数不仅可以属于所处的作用域，也可以属于某个代码块（通常指{ .. }内部）。 块作用域从 ES3 开始，try/catch 结构在 catch 分句中具有块作用域。 在 ES6 中引入了 let/const 关键字，用来在任意代码块中声明变量。if (..) { let a = 2; } 会声明一个劫持了 if 的{ .. }块的变量，并且将变量添加到这个块中。 有些人认为块作用域不应该完全作为函数作用域的替代方案。两种功能应该同时存在，开发者可以并且也应该根据需要选择使用何种作用域，创造可读、可维护的优良代码。 提升所有的声明（变量和函数）都会被“移动”到各自作用域的最顶端，这个过程被称为提升。 123456foo();function foo() { console.log(a); // undefined var a = 2;} 当你看到 var a = 2;时，可能会认为这是一个声明。但 JavaScript 实际上会将其看成两个声明：var a;和 a = 2;。第一个定义声明是在编译阶段进行的。第二个赋值声明会被留在原地等待执行阶段。实际是按照以下流程处理的： 12345var a;console.log(a);a = 2; 函数声明和变量声明都会被提升。但是一个值得注意的细节（这个细节可以出现在有多个“重复”声明的代码中）是函数会首先被提升，然后才是变量。 1234567891011foo(); // 会输出1而不是2！var foo;function foo() { console.log(1);}foo = function() { console.log(2);}; 声明本身会被提升，而包括函数表达式的赋值在内的赋值操作并不会提升。 123456foo(); // TypeErrorbar(); // ReferenceErrorvar foo = function bar() { // ...}; 要注意避免重复声明，特别是当普通的 var 声明和函数声明混合在一起的时候，否则会引起很多危险的问题！","link":"/blog/e91e243.html"},{"title":"《你不知道的JavaScript》笔记-闭包","text":"闭包就好像从 JavaScript 中分离出来的一个充满神秘色彩的未开化世界，只有最勇敢的人才能够到达那里。但实际上它只是一个普通且明显的事实，那就是我们在词法作用域的环境下写代码，而其中的函数也是值，可以随意传来传去。 当函数可以记住并访问所在的词法作用域，即使函数是在当前词法作用域之外执行，这时就产生了闭包。 闭包的实质当函数可以记住并访问所在的词法作用域时，就产生了闭包，即使函数是在当前词法作用域之外执行。 1234567891011function foo() { var a = 2; function bar() { console.log(a); // 2 } bar();}foo(); 这段代码看起来和嵌套作用域中的示例代码很相似。基于词法作用域的查找规则，函数 bar()可以访问外部作用域中的变量 a（这个例子中的是一个 RHS 引用查询）。这是闭包吗？技术上来讲，也许是。但根据前面的定义，确切地说并不是。我认为最准确地用来解释 bar()对 a 的引用的方法是词法作用域的查找规则，而这些规则只是闭包的一部分。（但却是非常重要的一部分！） 下面我们来看一段代码，清晰地展示了闭包： 12345678910111213function foo() { var a = 2; function bar() { console.log(a); } return bar;}var baz = foo();baz(); // 2 ———— 朋友，这就是闭包的效果。 函数 bar()的词法作用域能够访问 foo()的内部作用域。然后我们将 bar()函数本身当作一个值类型进行传递。在 foo()执行后，其返回值（也就是内部的 bar()函数）赋值给变量 baz 并调用 baz()，实际上只是通过不同的标识符引用调用了内部的函数 bar()。 bar()显然可以被正常执行。但是在这个例子中，它在自己定义的词法作用域以外的地方执行。 在 foo()执行后，通常会期待 foo()的整个内部作用域都被销毁，因为我们知道引擎有垃圾回收器用来释放不再使用的内存空间。由于看上去 foo()的内容不会再被使用，所以很自然地会考虑对其进行回收。 而闭包的“神奇”之处正是可以阻止这件事情的发生。事实上内部作用域依然存在，因此没有被回收。谁在使用这个内部作用域？原来是 bar()本身在使用。 拜 bar()所声明的位置所赐，它拥有涵盖 foo()内部作用域的闭包，使得该作用域能够一直存活，以供 bar()在之后任何时间进行引用。 bar()依然持有对该作用域的引用，而这个引用就叫作闭包。 当然，无论使用何种方式对函数类型的值进行传递，当函数在别处被调用时都可以观察到闭包。 12345678910111213function foo() { var a = 2; function baz() { console.log(a); // 2 } bar(baz);}function bar(fn) { fn(); // 妈妈快看呀，这就是闭包！} 传递函数当然也可以是间接的。 12345678910111213141516171819var fn;function foo() { var a = 2; function baz() { console.log(a); } fn = baz; // 将baz分配给全局变量}function bar() { fn(); // 妈妈快看呀，这就是闭包！}foo();bar(); // 2","link":"/blog/e3f0b8db.html"},{"title":"JWT小记","text":"Json web token (JWT), 是为了在网络应用环境间传递声明而执行的一种基于 JSON 的开放标准（(RFC 7519).该 token 被设计为紧凑且安全的，特别适用于分布式站点的单点登录（SSO）场景。JWT 的声明一般被用来在身份提供者和服务提供者间传递被认证的用户身份信息，以便于从资源服务器获取资源，也可以增加一些额外的其它业务逻辑所必须的声明信息，该 token 也可直接被用于认证，也可被加密。 token、cookie、session 的区别CookieCookie 总是保存在客户端中，按在客户端中的存储位置，可分为内存 Cookie 和硬盘 Cookie。 内存 Cookie 由浏览器维护，保存在内存中，浏览器关闭后就消失了，其存在时间是短暂的。硬盘 Cookie 保存在硬盘里，有一个过期时间，除非用户手工清理或到了过期时间，硬盘 Cookie 不会被删除，其存在时间是长期的。所以，按存在时间，可分为非持久 Cookie 和持久 Cookie。 cookie 是一个非常具体的东西，指的就是浏览器里面能永久存储的一种数据，仅仅是浏览器实现的一种数据存储功能。 cookie 由服务器生成，发送给浏览器，浏览器把 cookie 以 key-value 形式保存到某个目录下的文本文件内，下一次请求同一网站时会把该 cookie 发送给服务器。由于 cookie 是存在客户端上的，所以浏览器加入了一些限制确保 cookie 不会被恶意使用，同时不会占据太多磁盘空间，所以每个域的 cookie 数量是有限的。 Sessionsession 从字面上讲，就是会话。这个就类似于你和一个人交谈，你怎么知道当前和你交谈的是张三而不是李四呢？对方肯定有某种特征（长相等）表明他就是张三。 session 也是类似的道理，服务器要知道当前发请求给自己的是谁。为了做这种区分，服务器就要给每个客户端分配不同的“身份标识”，然后客户端每次向服务器发请求的时候，都带上这个“身份标识”，服务器就知道这个请求来自于谁了。至于客户端怎么保存这个“身份标识”，可以有很多种方式，对于浏览器客户端，大家都默认采用 cookie 的方式。 服务器使用 session 把用户的信息临时保存在了服务器上，用户离开网站后 session 会被销毁。这种用户信息存储方式相对 cookie 来说更安全，可是 session 有一个缺陷：如果 web 服务器做了负载均衡，那么下一个操作请求到了另一台服务器的时候 session 会丢失。 Tokentoken 的意思是“令牌”，是用户身份的验证方式，最简单的 token 组成:uid(用户唯一的身份标识)、time(当前时间的时间戳)、sign(签名，由 token 的前几位+盐以哈希算法压缩成一定长的十六进制字符串，可以防止恶意第三方拼接 token 请求服务器)。还可以把不变的参数也放进 token，避免多次查库","link":"/blog/9aac2df7.html"},{"title":"JavaScript面向对象有感","text":"面向对象程序设计（简称 OOP）是现在最流行的程序设计方法,JavaScript 的核心是支持面向对象的，Prototype 是 JavaScript 实现与管理继承的一种机制。最近在读《你不知道的 JavaScript》一书中了解到除了使用类实现外，还可以用一种“对象关联”（OLOO，objects linked to other objects）的编程风格 在 JavaScript 中定义一个类的传统方法是通过构造函数。可以这样写： 12345678910111213141516171819202122232425262728293031function Foo(name) { this.name = name;}Foo.prototype.getName = function() { console.log(`name:${this.name}`);};function Bar(name, age) { Foo.call(this, name); this.age = age;}Bar.prototype = Object.create(Foo.prototype);Bar.prototype.constructor = Bar;// 或者使用es6新增的setPrototypeOf方法// Object.setPrototypeOf(Bar.prototype, Foo.prototype);Bar.prototype.speak = function() { console.log(`My name is ${this.name}, age ${this.age}`);};const b1 = new Bar('Zubin', 10);const b2 = new Bar('Jake', 15);b1.getName(); // name:Zubinb1.speak(); // My name is Zubin, age 10b2.getName(); // name:Jakeb2.speak(); //My name is Jake, age 15 ES6 提供了更接近传统语言的写法，引入了 Class（类）这个概念，作为对象的模板。通过 class 关键字，可以定义类。 12345678910111213141516171819202122232425262728class Foo { constructor(name) { this.name = name; } getName() { console.log(`name:${this.name}`); }}class Bar extends Foo { constructor(name, age) { super(name); this.age = age; } speak() { console.log(`My name is ${this.name}, age ${this.age}`); }}const b1 = new Bar('Zubin', 10);const b2 = new Bar('Jake', 15);b1.getName(); // name:Zubinb1.speak(); // My name is Zubin, age 10b2.getName(); // name:Jakeb2.speak(); // My name is Jake, age 15 最后是《你不知道的 JavaScript》一书中作者提倡的一种 OLOO 的风格，使用 Object.create() 方法直接生成实例。 12345678910111213141516171819202122232425262728const Foo = { init(name) { this.name = name; }, getName() { console.log(`name:${this.name}`); },};const Bar = Object.create(Foo);Bar.init = function(name, age) { this.name = name; this.age = age;};Bar.speak = function() { console.log(`My name is ${this.name}, age ${this.age}`);};const b1 = Object.create(Bar);b1.init('Zubin', 10);const b2 = Object.create(Bar);b2.init('Jake', 15);b1.getName(); // name:Zubinb1.speak(); // My name is Zubin, age 10b2.getName(); // name:Jakeb2.speak(); //My name is Jake, age 15 从书里的描述来看，作者的观点是 OLOO 总比 Class 的形式要好。如果你只读这本书，那么结论必然是我们不需要传统的 class 形式。个人认为 Class 未必就像书中所说的那样不堪，现实中有几点需要考虑： Class 在构建对象实例时使用约定成俗的 new 方法，风格更加统一，OLOO 写法中的 init 方法是自己定义的 123456// classvar f = new Foo(123, 'abc'); // 标准//OLOOvar f = Object.create(Foo); // 需要两个步骤f.init(123, 'abc'); // init 并不是 JS 中的标准函数，而是 Foo 中自定义的 基于 Class 的面向对象的写法是“主流”，毕竟在项目中代码不是你一个人看的。 Class 对于大部分人（尤其是后端程序员）更容易接受。 Class 是基于 prototype 更为严格和易用的语法糖,目的是为了 改善 原来 prototype 丑和难用，但是 C++ 和 Java 中的类 跟 js 中的类不是一回事 它们的继承，封装，多态在 js 的类中是不存在的。当然利用 TypeScript 的 Interface 来实现“面向 Interface 编程”也不错的选择。","link":"/blog/6c64024e.html"},{"title":"MongoDB固定集合","text":"MongoDB 固定集合（Capped Collections）是性能出色且有着固定大小的集合，大小固定，有点类似数据结构中的循坏队列，先进先出方式。满队列删除原来的元素！ 常用命令创建固定集合： 1db.createCollection(“test”,{capped:true,size:20000,max:2000}) size 是整个集合空间大小，单位为【KB】 max 是集合文档个数上线，单位是【个】 判断集合是否为固定集合: 1db.test.isCapped() 如果需要将已存在的集合转换为固定集合可以使用以下命令： 1db.runCommand({&quot;convertToCapped&quot;:&quot;test&quot;,size:10000}) 固定集合文档按照插入顺序储存的,默认情况下查询就是按照插入顺序返回的,也可以使用$natural 调整返回顺序。 1db.test.find().sort({$natural:-1}) 固定集合特点 对固定集合进行插入速度极快 按照插入顺序的查询输出速度极快 能够在插入最新数据时,淘汰最早的数据 固定集合缺点固定集合无法精确到具体的时间，文档的条数与文档大小都不能确定。固定集合无法分片，对于需要经常查询的可能就不是太合适 应用场景在数据量较少的情况下，或者只需要保存部分的数据，如 储存日志信息用法 缓存一些少量的文档","link":"/blog/813afec1.html"},{"title":"理解并发和并行的区别","text":"并发是什么？并发是指同一时刻只能处理一个任务，但一个时间段内可以对多个任务交替处理 。一个例子就能很好地说明这一点。 我们可以想象一个人正在跑步。假如在他晨跑时，鞋带突然松了。于是他停下来，系一下鞋带，接下来继续跑。这个例子就是典型的并发。这个人能够一下搞定跑步和系鞋带两件事，即立即处理多个任务。 并行是什么？并行和并发有何区别？并行是指同一时刻可以处理多个任务 。这听起来和并发差不多，但其实完全不同。 我们同样用这个跑步的例子来帮助理解。假如这个人在慢跑时，还在用他的 iPod 听着音乐。在这里，他是在跑步的同时听音乐，也就是同时处理多个任务。这称之为并行。","link":"/blog/13be4694.html"},{"title":"Vim常用命令备忘","text":"vim 常用命令备忘，不定时更新。 批量替换123456# 把文件内abc替换成123:%s/abc/123/g# 把20行到30行内abc替换成123:20,30s/abc/123 跳转 跳到文本的最后一行：按“G”,即“shift+g” 跳到最后一行的最后一个字符 ： 先重复 1 的操作即按“G”，之后按“$”键，即“shift+4”。 跳到第一行的第一个字符：先按两次“g”， 跳转到当前行的第一个字符：在当前行按“0”。 查找命令模式下，输入/&lt;要查询的字符串&gt;按下 n 查看下一个匹配，N 查看上一个匹配","link":"/blog/4dee9084.html"},{"title":"golang函数参数传递详解","text":"参数传递是指在程序的传递过程中，实际参数就会将参数值传递给相应的形式参数，然后在函数中实现对数据处理和返回的过程。比较常见的参数传递有：值传递、指针传递、引用传递。 一直以为 Go 里面函数传参有值传递和引用传递两种方式，对指针传递和引用传递的区别也不是很清楚，查看了下官方文档才发现并不是这么回事。 In a function call, the function value and arguments are evaluated in the usual order. After they are evaluated, the parameters of the call are passed by value to the function and the called function begins execution. 文档地址：https://golang.org/ref/spec#Calls或者：http://docs.studygolang.com/ref/spec#Calls 官方文档已经明确说明：Go 里边函数传参只有值传递一种方式，为了加强自己的理解，再把每种传参方式进行一次梳理。 值传递 值传递是指在调用函数时将实际参数复制一份传递到函数中，这样在函数中如果对参数进行修改，将不会影响到实际参数。 运行以下代码： 12345678910111213func inc(x int) { fmt.Printf(&quot;x 内存地址:%p 值:%d \\n&quot;, &amp;x, x) x++ fmt.Printf(&quot;x:%d \\n&quot;, x)}func main() { a := 1 fmt.Printf(&quot;a 内存地址:%p 值:%d \\n&quot;, &amp;a, a) inc(a) fmt.Printf(&quot;执行后 a:%d \\n&quot;, a)} 执行结果为： 1234a 内存地址:0xc42008c008 值:1x 内存地址:0xc42008c018 值:1x:2执行后 a:1 可以看出程序中使用的是值传递，形参 x 是实参 a 在栈上的一份拷贝， 和实参拥有两个完全不同的地址，在函数内部改变了 x 的值，a 并没有改变 指针传递 形参为指向实参地址的指针，当对形参的指向操作时，就相当于对实参本身进行的操作。 修改上面代码 12345678910111213func inc(x *int) { fmt.Printf(&quot;x 内存地址:%p 值:%p \\n&quot;, &amp;x, x) *x++ fmt.Printf(&quot;x:%d \\n&quot;, *x)}func main() { a := 1 fmt.Printf(&quot;a 内存地址:%p 值:%d \\n&quot;, &amp;a, a) inc(&amp;a) fmt.Printf(&quot;执行后 a:%d \\n&quot;, a)} 执行结果为： 1234a 内存地址:0xc42008c008 值:1x 内存地址:0xc4200a0020 值:0xc42008c008x:2执行后 a:2 可以看到指针&amp;a 传给函数的形参 x 后，形参将会是它在栈上的一份拷贝，他们本身将各自拥有不同的地址，但是二者的值是一样的（都是变量 a 的地址），因此可以通过指针相关的操作来改变 a 的值。 引用传递 引用传递是指在调用函数时将实际参数的地址传递到函数中，那么在函数中对参数所进行的修改，将影响到实际参数 由于 Go 中并不存在引用传递，我们看下一段 c++代码 1234567891011121314void inc( int&amp; x){ printf(&quot;x 内存地址:%p 值:%d \\n&quot;, &amp;x, x); x++; printf(&quot;x:%d \\n&quot;, x);}int main(int argc, const char * argv[]) { // insert code here... int a=1; printf(&quot;a 内存地址:%p 值:%d \\n&quot;, &amp;a, a); inc(a); printf(&quot;执行后 a:%d \\n&quot;, a); return 0;} 执行结果： 1234a 内存地址:0x7ffeefbff57c 值:1x 内存地址:0x7ffeefbff57c 值:1x:2执行后 a:2 可以看到引用传递，操作地址就是实参地址 ，只是相当于实参的一个别名，对它的操作就是对实参的操作 那么传 slice 是不是传引用呢？执行下面代码 123456789101112131415161718192021func change(x []int) { fmt.Printf(&quot;\\nx: %#v x内存地址:%p \\n&quot;, x, &amp;x) for i, _ := range x { fmt.Printf(&quot;x[%d]内存地址：%p\\n&quot;, i, &amp;x[i]) } x[1] = 8 fmt.Printf(&quot;\\n执行后 x:%#v \\n&quot;, x)}func main() { a := []int{1, 2, 3} fmt.Printf(&quot;a: %#v a内存地址:%p \\n&quot;, a, &amp;a) for i, _ := range a { fmt.Printf(&quot;a[%d]内存地址：%p\\n&quot;, i, &amp;a[i]) } change(a) fmt.Printf(&quot;\\n执行后 a:%#v \\n&quot;, a)} 执行结果为： 12345678910111213a: []int{1, 2, 3} a内存地址:0xc42000a0c0a[0]内存地址：0xc4200182c0a[1]内存地址：0xc4200182c8a[2]内存地址：0xc4200182d0x: []int{1, 2, 3} x内存地址:0xc42000a100x[0]内存地址：0xc4200182c0x[1]内存地址：0xc4200182c8x[2]内存地址：0xc4200182d0执行后 x:[]int{1, 8, 3}执行后 a:[]int{1, 8, 3} 可以看出形参和实参拥有不同的地址，但是它们存储的地址是一样的，这也是能够进行修改的原因，所以说传 slice 不是传引用*显而易见Go 里边函数传参只有值传递一种方式**, 包括 slice/map/chan 在内所有类型, 没有传引用的说法. 完整代码","link":"/blog/c1aebfdc.html"},{"title":"CAP定理、ACID模型、BASE模型","text":"CAP 定理在理论计算机科学中，CAP 定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点： 一致性（Consistency)：所有节点在同一时间具有相同的数据 可用性（Availability）：保证每个请求不管成功或者失败都有响应 分隔容忍（Partition tolerance）：系统中任意信息的丢失或失败不会影响系统的继续运作根据定理，分布式系统只能满足三项中的两项而不可能满足全部三项。 ACID 模型：ACID，是指数据库管理系统（DBMS）在写入/异动资料的过程中，为保证交易（transaction）是正确可靠的，所必须具备的四个特性： 原子性（Atomicity）：要么整个事务成功，要么整个不成功 一致性（Consistency）：数据库在事务之间处于一个一致的状态中 隔离性（Isolation）：又称独立性,在其他事务结束之前，事务看不到被它们更改的数据 持久性（Durability）：一旦数据库系统通知用户事务成功，数据就永不丢失 BASE 模型反 ACID 模型，完全不同 ACID 模型，牺牲高一致性，获得可用性或可靠性： 基本可用(Basically Available)：支持分区失败(e.g. sharding 碎片划分数据库) 软状态(Soft state)：状态可以有一段时间不同步，异步。 最终一致(Eventually consistent)：最终数据是一致的就可以了，而不是时时高一致。 BASE 思想的主要实现有： 按功能划分数据库 sharding 碎片","link":"/blog/851fe1dd.html"},{"title":"Chrome下载文件出错","text":"使用谷歌浏览器下载文件时发生浏览器崩溃,页面提示:ERR_RESPONSE_HEADERS_MULTIPLE_CONTENT_DISPOSITION 原因：加载的资源名含有半角逗号（,）或者别的特殊符号 解决办法： 替换半角逗号等特殊符号 服务器端函数响应中若设置了“content-disposition”响应头,filename参数使用双引号包裹。 koa 示例： 1ctx.set('Content-Disposition', `attachment;filename=&quot;${encodeURI(fileName)}.txt&quot;`);","link":"/blog/e6d7f35f.html"},{"title":"MVC、MVP、MVVM联系和区别","text":"MVC、MVP、MVVM 这些模式是为了解决开发过程中的实际问题而提出来的，目前作为主流的几种架构模式而被广泛使用。 MVCMVC（Model-View-Controller） 是表现层的框架模式，用于解除业务逻辑和视图之间的耦合，从而易于扩展，便于测试。MVC 模式认为软件可以分成三个部分。 视图（View）：用户界面。 控制器（Controller）：业务逻辑 模型（Model）：数据保存 用户操作-&gt;View（负责接收用户的输入操作）-&gt;Controller（业务逻辑处理）-&gt;Model（数据持久化）-&gt;View（将结果反馈给 View）。 优点： 分层，结构清晰，耦合性低，大型项目代码的复用性得到极大的提高. 开发人员分工明确，提高了开发的效率，维护方便，降低了维护成本。 缺点： 增加了系统结构和实现的复杂性 视图与控制器间的过于紧密的连接 视图对模型数据的低效率访问 目前一般高级的界面工具或构造器不支持 MVC 模式 MVPMVP 模式将 Controller 改名为 Presenter，同时改变了通信方向。目的就是为了完全切断 View 跟 Model 之间的联系，由 Presenter 充当桥梁，做到 View-Model 之间通信的完全隔离. 主要的特点是： 各部分之间的通信，都是双向的。 View 与 Model 不发生联系，都通过 Presenter 传递。 View 非常薄，不部署任何业务逻辑，称为”被动视图”（Passive View），即没有任何主动性，而 Presenter 非常厚，所有逻辑都部署在那里。 优点： 便于测试。Presenter 对 View 是通过接口进行，在对 Presenter 进行不依赖 UI 环境的单元测试的时候。可以通过 Mock 一个 View 对象，这个对象只需要实现了 View 的接口即可。然后依赖注入到 Presenter 中，单元测试的时候就可以完整的测试 Presenter 业务逻辑的正确性。这里根据上面的例子给出了 Presenter 的单元测试样例。 View 可以进行组件化。在 MVP 当中，View 不依赖 Model。这样就可以让 View 从特定的业务场景中脱离出来，可以说 View 可以做到对业务逻辑完全无知。它只需要提供一系列接口提供给上层操作。这样就可以做高度可复用的 View 组件。 缺点： Presenter 中除了业务逻辑以外，还有大量的 View-&gt;Model，Model-&gt;View 的手动同步逻辑，造成 Presenter 比较笨重，维护起来会比较困难。 MVVMMVVM 模式将 Presenter 改名为 ViewModel，基本上与 MVP 模式完全一致。 唯一的区别是，它采用双向绑定（data-binding）：View 的变动，自动反映在 ViewModel，反之亦然。Angular 和 Ember 都采用这种模式。 典型的应用有.NET 的 WPF，js 框架 Knockout、AngularJS 、Vue 等。 优点： 提高可维护性。解决了 MVP 大量的手动 View 和 Model 同步的问题，提供双向绑定机制。提高了代码的可维护性。 简化测试。因为同步逻辑是交由 Binder 做的，View 跟着 Model 同时变更，所以只需要保证 Model 的正确性，View 就正确。大大减少了对 View 同步更新的测试。 缺点： 过于简单的图形界面不适用，或说牛刀杀鸡。 对于大型的图形应用程序，视图状态较多，ViewModel 的构建和维护的成本都会比较高。 数据绑定的声明是指令式地写在 View 的模版当中的，这些内容是没办法去打断点 debug 的。 参考资料http://www.ruanyifeng.com/blog/2015/02/mvcmvp_mvvm.htmlhttps://www.jianshu.com/p/6a86f7fdc0cb","link":"/blog/32d4c527.html"},{"title":"panic和recover","text":"在 Go 语言中，程序中一般是使用错误来处理异常情况。对于程序中出现的大部分异常情况，错误就已经够用了。 但在有些情况，当程序发生异常时，无法继续运行。在这种情况下，我们会使用 panic 来终止程序。当函数发生 panic 时，它会终止运行，在执行完所有的延迟函数后，程序控制返回到该函数的调用方。这样的过程会一直持续下去，直到当前协程的所有函数都返回退出，然后程序会打印出 panic 信息，接着打印出堆栈跟踪（Stack Trace），最后程序终止。在编写一个示例程序后，我们就能很好地理解这个概念了。 当程序发生 panic 时，使用 recover 可以重新获得对该程序的控制。 可以认为 panic 和 recover 与其他语言中的 try-catch-finally 语句类似，只不过一般我们很少使用 panic 和 recover。而当我们使用了 panic 和 recover 时，也会比 try-catch-finally 更加优雅，代码更加整洁。 panic 的应用场景需要注意的是，你应该尽可能地使用错误，而不是使用 panic 和 recover。只有当程序不能继续运行的时候，才应该使用 panic 和 recover 机制。 panic 有两个合理的用例。 发生了一个不能恢复的错误，此时程序不能继续运行。 一个例子就是 web 服务器无法绑定所要求的端口。在这种情况下，就应该使用 panic，因为如果不能绑定端口，啥也做不了。 发生了一个编程上的错误。 假如我们有一个接收指针参数的方法，而其他人使用 nil 作为参数调用了它。在这种情况下，我们可以使用 panic，因为这是一个编程错误：用 nil 参数调用了一个只能接收合法指针的方法。 panic 示例当程序终止时，会打印传入 panic 的参数,接着打印出堆栈跟踪。 12345678910111213141516171819202122package mainimport ( &quot;fmt&quot;)func fullName(firstName *string, lastName *string) { if firstName == nil { panic(&quot;runtime error: first name cannot be nil&quot;) } if lastName == nil { panic(&quot;runtime error: last name cannot be nil&quot;) } fmt.Printf(&quot;%s %s\\n&quot;, *firstName, *lastName) fmt.Println(&quot;returned normally from fullName&quot;)}func main() { firstName := &quot;Elon&quot; fullName(&amp;firstName, nil) fmt.Println(&quot;returned normally from main&quot;)} 运行该程序，会有如下输出： 1234567panic: runtime error: last name cannot be nilgoroutine 1 [running]:main.fullName(0x1040c128, 0x0) /tmp/sandbox135038844/main.go:12 +0x120main.main() /tmp/sandbox135038844/main.go:20 +0x80 recover 示例recover 是一个内建函数，用于重新获得 panic 协程的控制。 123456789101112131415161718192021222324252627282930package mainimport ( &quot;fmt&quot;)func recoverName() { if r := recover(); r!= nil { fmt.Println(&quot;recovered from &quot;, r) }}func fullName(firstName *string, lastName *string) { defer recoverName() if firstName == nil { panic(&quot;runtime error: first name cannot be nil&quot;) } if lastName == nil { panic(&quot;runtime error: last name cannot be nil&quot;) } fmt.Printf(&quot;%s %s\\n&quot;, *firstName, *lastName) fmt.Println(&quot;returned normally from fullName&quot;)}func main() { defer fmt.Println(&quot;deferred call in main&quot;) firstName := &quot;Elon&quot; fullName(&amp;firstName, nil) fmt.Println(&quot;returned normally from main&quot;)} 当程序在第 19 行发生 panic 时，会调用延迟函数 recoverName，它反过来会调用 recover() 来重新获得 panic 协程的控制。第 8 行调用了 recover，返回了 panic 的传参，因此会打印： 123recovered from runtime error: last name cannot be nilreturned normally from maindeferred call in main 注意：只有在相同的 Go 协程中调用 recover 才管用。recover 不能恢复一个不同协程的 panic。","link":"/blog/115dd571.html"},{"title":"git常用命令总结","text":"Git 常用命令 创建版本库12git clone &lt;url&gt; #克隆远程版本库git init #初始化本地版本库 修改和提交123456789git status #查看状态git diff #查看变更内容git add . #跟踪所有改动过的文件git add &lt;file&gt; #跟踪指定的文件git mv &lt;old&gt;&lt;new&gt; #文件改名git rm &lt;file&gt; #删除文件git rm --cached &lt;file&gt; #停止跟踪文件但不删除git commit -m 'commit message' #提交所有变更过的文件git commit --amend # 修改最后一次提交 查看提交历史123git log #查看提交历史git log -p &lt;file&gt; #查看指定文件的提交历史git blame &lt;file&gt; # 以列表方式查看指定文件的提交历史 撤销123git reset --hard HEAD #撤销工作目录中所有未提交文件的修改内容git checkout HEAD &lt;file&gt; #撤销指定未提交文件的修改内容git revert &lt;commit&gt; #撤销指定的提交 分支与标签12345678git branch # 显示所有本地分支git checkout &lt;branch/tag&gt; # 切换到指定分支或者标签git branch &lt;new branch&gt; #创建新的分支git branch -d &lt;branch&gt; #删除本地分支git branch -m old new #本地分支重命名git tag #列出所有本地标签git tag &lt;tag&gt; # 基于最新提交创建标签git tag -d &lt;tag&gt; # 删除标签 撤销删除分支操作 git reflog 查看你上一次 commit SHA1值,根据 你的SHA1值，创建一个分支 1git branch &lt;branchName&gt; &lt;sha1&gt; 你删除 分支的时候 git branch -D &lt;branchName&gt; 后面会有 SHA1值 1Deleted branch development (was 130d7ba). 利用这个SHA1值 就可以恢复你的 分支 1git branch development 130d7ba 合并与衍合123git merge &lt;branch&gt; #合并指定分支到当前分支git rebase &lt;branch&gt; #衍合指定分支到当前分支git rebase -i # 修改提交历史 远程操作123456789git remote -v # 查看远程版本库信息git remote show &lt;remote&gt; #查看指定远程版本库信息git remote add &lt;remote&gt; &lt;url&gt; #添加远程版本库git fetch &lt;remote&gt; # 从远程库获取代码git pull &lt;remote&gt; &lt;branch&gt; #下载代码及快速合并git push &lt;remote&gt; &lt;branch/tag&gt; #上传代码及快速合并git push --delete origin &lt;branch&gt; # 刪除远程分支不含remotes/origin）git push --all # 上传所有分支代码git push --tags # 上传所有分支","link":"/blog/acc3d3b0.html"},{"title":"git操作失败小结","text":"git process semms to be running in this repositoryGit 提交的过程中突然显示 Another git process semms to be running in this repository, e.g. an editor opened by ‘git commit’. Please make sure all processes are terminated then try again. If it still fails, a git process remove the file manually to continue…即是 git 被另外一个程序占用。 原因在于 Git 在使用过程中遭遇了奔溃，部分被上锁资源没有被释放导致的。 解决方案：进入项目文件夹下的 .git 文件中（显示隐藏文件夹或 rm .git/index.lock）删除 index.lock 文件即可。","link":"/blog/613c929d.html"},{"title":"数据库设计-约束","text":"数据库约束是为了保证数据的完整性(正确性)而实现的一套机制。主要分为五大约束： 主键约束（Primay Key Coustraint）： 唯一性，非空性 123# 添加主键约束（将UserId作为主键）alter table UserId add constraint PK_UserId primary key (UserId); 唯一约束 （Unique Counstraint）：唯一性，可以空，但只能有一个 123# 添加唯一约束（身份证号唯一，因为每个人的都不一样）alter table UserInfo add constraint UQ_IDNumber unique(IdentityCardNumber); 检查约束 （Check Counstraint）：对该列数据的范格式的限制（如：年性别等） 12345# 添加检查约束 （对年龄加以限定 20-40 岁之间）alter table UserInfo add constraint CK_UserAge check (UserAge between 20 and 40);alter table UserInfo add constraint CK_UserSex check (UserSex=’男’ or UserSex=’女′); 默认约束 （Default Counstraint）：该数据的默认值 123# 添加默认约束（如果地址不填 默认为“地址不详”）alter table UserInfo add constraint DF_UserAddress default (‘地址不详’) for UserAddress; 外键约束 （Foreign Key Counstraint）：需要建立两表间的关系并引用主表的列 123# 添加外键约束 (主表 UserInfo 和从表 UserOrder 建立关系，关联字段 UserId)alter table UserOrder add constraint FK_UserId_UserId foreign key(UserId)references UserInfo(UserId);","link":"/blog/adc3b0ca.html"},{"title":"Redis基础数据类型","text":"Redis 有 5 种基础数据结构，分别为：String (字符串)、Hash (哈希)、List (列表)、Set (集合) 和 Sorted Set (有序集合) String (字符串)String 数据结构是简单的 key-value 类型。最常见的用途就是缓存信息。我们可以将需要缓存结构体使用 JSON 序列化成字符串来缓存，取出来的时候再反序列化一下。 常用命令 说明 示例 set 设置指定 key 的值 set name zubin get 获取指定 key 的值 get name del 删除指定 key del key mset 批量设置指定 key 的值 mset name zubin age 18 mget 批量获取指定 key 的值 mget name age setex 如果设置指定 key 的值和过期时间(秒) setex name 5 zubin setnx 只有 key 不存在时设置 key 的值 setnx name zubin incr 将 key 中存储的数字值增 1 incr age incyby 将 key 所储存的值加上给定的增量值 incrby age 5 decr 将 key 中存储的数字值减 1 decr age strlen 返回指定 key 中 value 的长度 strlen name append 将字符串追加到指定 key 中值的末尾。 append name 1 Hash (哈希)Redis hash 是一个 string 类型的 field 和 value 的映射表，是无序字典, 特别适合用于存储对象。每个 hash 可以存储2^32 - 1（4294967295） 键值对。 常用命令 说明 示例 hset 设置 hash 表中 field 的值 hset user name zubin hmset 设置 hash 表中多个 field 的值 hmset user name zubin age 18 hsetnx field 不存在时设置哈希表字段的值 hsetnx user name zubin hget 获取指定字段的值 hget user name hmget 获取多个字段的值 hmget user age name hgetall 获取 hash 表中所有字段的值 hgetall user hdel 删除一个或多个 hash 字段 hdel user age hexists 判断 hash 表中指定的字段是否存在 hexists user age hincrby 为 hash 表中指定的整数值字段加上增量 hincrby user age 5 hkeys 获取 hash 表中所有的字段 hkeys user hvals 获取 hash 表中所有的值 hvals user hlen 获取 hash 表中字段的数量 hlen user List (列表)Redis 列表是简单的字符串列表，按照插入顺序排序，注意它是链表而不是数组，一个列表最多可以包含 2^32 - 1 个元素 (4294967295, 每个列表超过 40 亿个元素)。list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)。list 常用来做异步队列使用，你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 常用命令 说明 示例 lpush 将一个或多个值插入到列表头部 lpush skills Node Golang Java lpushx 将一个或多个值插入到已存在的列表头部 lpushx skills Node Golang Java lpop 移除列表第一个元素 lpop skills blpop 移除列表第一个元素， 列表为空会阻塞，直到超时或者有可移除的元素 blpop skills 100 rpush 将一个或多个值插入到列表尾部 rpush skills Node Golang Java rpushx 将一个或多个值插入到已存在的列表尾部 rpushx skills Node Golang Java rpop 移除列表最后一个个元素 rpop skills brpop 移除列表最后一个个元素， 列表为空会阻塞，直到超时或者有可移除的元素 brpop skills 100 brpoplpush 从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它 brpoplpush skills list1 500 lindex 通过索引获取列表中的元素 lindex skills 1 lset 通过索引来设置元素的值 lset skills 0 Python llen 获取列表长度 llen skills lrange 获取列表指定范围内的元素 lrange skills 1 2 lrem 根据参数 COUNT 的值，移除列表中与参数 VALUE 相等的元素 lrem skills 0 node ltrim 修剪列表，只保留指定区间内的元素 ltrim skills 1 2 Set (集合)Redis 的 Set 是 String 类型的无序集合。它内部的键值对是无序的唯一的，这就意味着集合中不能出现重复的数据，集合中最大的成员数为 2^32 - 1 (4294967295)。Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 常用命令 说明 示例 sadd 向集合添加一个或多个元素 sadd skills python go java go scard 获取集合的成员数 scard skills sismember 判断是否集合中的成员 sismember skills go smembers 返回集合中的所有成员 smembers skills spop 随机移除并返回集合中的一个元素 spop skills sinter 返回指定集合的交集 sinter key1 key2 sunion 返回指定集合的并集 sunion key1 key2 sdiff 返回指定集合的差集 sdiff key2 key2 Sorted Set (有序集合)Redis 有序集合和集合一样也是 string 类型元素的集合,一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score（分数），代表这个 value 的排序权重。集合中最大的成员数为 2^32 - 1(4294967295)。Redis 正是通过分数来为集合中的成员进行从小到大的排序，有序集合的成员是唯一的,但分数(score)却可以重复。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 常用命令 说明 示例 zadd 添加一个或多个成员或者更新已存在成员的分数 zadd userscore zubin 1 zcard 获取有序集合的成员数 zcard userscore zcount 获取有序集合指定区间分数的成员数 zcount userscore 1 3 zincrby 给指定成员的分数加上增量 zincrby userscore 1 zubin zrange 返回指定区间内的成员 zrange userscore 0 -1 zrangebyscore 返回 score 区间内的成员 zrangebyscore userscore -inf +inf zrem 移除一个或多个元素 zrem userscore 0 -1 zrevrange 返回指定分数区间内的成员,按分数递减 zrevrange userscore 0 -1 zrevrank 返回成员 member 的排名 zrevrank userscore zubin zscore 返回 score 的 score 值 zscore userscore zubin","link":"/blog/553331a9.html"},{"title":"数据库设计-范式","text":"范式是符合某一种级别的关系模式的集合。关系型数据库中的关系必须满足一定的要求，满足不同程度要求的为不同范式。目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、Boyce-Codd 范式（BCNF）、第四范式（4NF）和第五范式（5NF）。范式越高，冗余最低，一般到三范式即可，再往上，表越多，可能导致查询效率下降。所以有时为了提高运行效率，可以让数据冗余 一、第一范式（1NF）：确保每列的原子性数据表中的每一列（每个字段）必须是不可拆分的最小单元 用户 ID 姓名 电话 10001 张三 0755-88888888 18888888888 上面的电话字段可以继续分固定电话和手机 用户 ID 姓名 固定电话 手机 10001 张三 0755-88888888 18888888888 二、第二范式(2NF)：非键字段必须依赖于键字段如果一个关系满足 1NF，并且除了主键以外的其它列，都依赖于该主键。也就是说在一个数据库表中必须有一个主键，没有包含在主键中的列必须完全依赖于主键，而不能只依赖于主键的一部分。 比如说要设计一个订单信息表，所以要将订单编号和商品编号作为数据库的联合主键 订单编号 商品编号 商品名称 数量 000001 p00001 宝马 1 商品名称跟订单编号无关而仅仅跟商品编号有关，这里违反了 2NF 的设计原则，会存在如下问题： 数据冗余：一个产品销售了 n 次，“商品名称”就重复 n 次。 更新异常：若调整了某商品的名称，数据表中所有行的“商品名称”值都要更新，否则会出现同一个商品名称不同的情况。 插入异常：假设要新增新的商品，暂时还没有订单。这样，由于还没有“订单编号”关键字，“商品编号”和“商品名称”也无法记录入数据库。 删除异常：假设要一批商品，这些商品信息就应该从数据库表中删除，与此同时，销售信息也被删除了。 所以上面的表实际上可以拆分为 订单编号 商品编号 数量 000001 p00001 1 商品编号 商品名称 p00001 宝马 三、第三范式(3NF)：确保每列都和主键列直接相关,而不是间接相关在第二范式的基础上，数据表中如果不存在非关键字段对任一候选关键字段的传递函数依赖则符合第三范式。满足第三范式的数据库表应该不存在如下依赖关系： 1关键字段 → 非关键字段x → 非关键字段y 比如设计订单信息表 订单编号 商品编号 数量 业务员 Id 业务员姓名 000001 p00001 1 10001 张三 明显有 业务员姓名 -&gt; 业务员 Id -&gt; 订单编号 的依赖关系，它也会存在 数据冗余、更新异常、插入异常和 删除异常 的情况，上面表可以拆分为： 订单编号 商品编号 数量 业务员 Id 000001 p00001 1 10001 业务员 Id 业务员姓名 10001 张三 这样的数据库表是符合第三范式的，消除了 数据冗余、更新异常、插入异常和 删除异常。 四、Boyce-Codd 范式（BCNF）：主属性不依赖于主属性在第三范式的基础上，数据库表中如果不存在任何字段对任一候选关键字段的传递函数依赖则符合第三范式。假设仓库管理关系表为： 仓库编号 商品编号 管理员编号 数量 000001 000001 10001 10 假设一个管理员只在一个仓库工作；一个仓库可以存储多种物品。这个数据库表中存在如下决定关系： 12(仓库编号,商品编号) →(管理员编号,数量)(管理员编号,商品编号) → (仓库编号,数量) 所以，(仓库编号,商品编号)和(管理员编号,商品编号)都是候选关键字，表中的唯一非关键字段为数量，它是符合第三范式的。但是，由于存在如下决定关系： 12(仓库编号) → (管理员编号)(管理员编号) → (仓库编号) 删除异常：当仓库被清空后，所有”商品编号”和”数量”信息被删除的同时，”仓库编号”和”管理员编号”信息也被删除了。 插入异常：当仓库没有存储任何物品时，无法给仓库分配管理员。 更新异常：如果仓库换了管理员，则表中所有行的管理员编号 都要修改。 把仓库管理关系表分解为二个关系表： 仓库编号 商品编号 数量 000001 000001 10 仓库编号 管理员编号 10001 10001 这样的数据库表是符合 BCNF 范式的，消除了删除异常、插入异常和更新异常。 还可以这么说：若一个关系达到了第三范式，并且它只有一个候选码，或者它的每个候选码都是单属性，则该关系自然达到 BCNF。 五、 第四范式(4NF)：要求把同一表内的多对多关系删除。当一个表中的非主属性互相独立时（3NF），这些非主属性不应该有多值。若有多值就违反了第四范式。 下面用户表符合 3NF ，但在某些情况下，比如说有两个手机号码时，这样的表还是不合理的，违反了 4NF。 用户 ID 姓名 固定电话 手机 10001 张三 0755-88888888 18888888888 10001 张三 0755-88888888 18888888889 就可以拆分为固定电话表(用户 ID，固定电话)和手机号码表(用户 ID，手机号码) 所以只有在某些特殊情况下，要考虑将表规范到第四范式。 六、 第五范式(5NF)：最终范式。满足第四范式条件下，表必须可以分解为较小的表，除非那些表在逻辑上拥有与原始表相同的主键。比如一个销售信息表（销售人员，供应商，产品）。在某些情况下，这个表中会产生一些冗余。可以将表分解为： 销售人员供应商表（销售人员，供应商）； 销售人员产品表（销售人员，产品）； 供应商产品表（供应商，产品）","link":"/blog/423b70da.html"},{"title":"MongoDB-aggregate用法","text":"MongoDB 中聚合(aggregate)方法可以对集合中的文档进行变换和组合，主要用于处理数据。语法： 1db.collection.aggregate(pipeline, options); 管道操作符MongoDB 的聚合管道将 MongoDB 文档在一个管道处理完毕后将结果传递给下一个管道处理，管道操纵是可以重复的。 管道聚合阶段: 1234567$project：包含、排除、重命名和显示字段$match：查询，需要同find()一样的参数$limit：限制结果数量$skip：忽略结果的数量$sort：按照给定的字段排序结果$group：按照给定表达式组合结果$unwind：分割嵌入数组到自己顶层文件&lt;br&gt; group 查询操作符： 12345678$sum 总结从集合中的所有文件所定义的值.$avg 从所有文档集合中所有给定值计算的平均.$min 获取集合中的所有文件中的相应值最小.$max 获取集合中的所有文件中的相应值的最大.$push 值插入到一个数组生成文档中.$addToSet 值插入到一个数组中所得到的文档，但不会创建重复.$first 根据分组从源文档中获取的第一个文档。通常情况下，这才有意义，连同以前的一些应用 “$sort”-stage.$last 根据分组从源文档中获取最后的文档。通常，这才有意义，连同以前的一些应用 “$sort”-stage. 附加选项 explain：布尔值，指定返回结果是否显示该操作的执行计划 allowDiskUse：布尔值，指定该聚合操作是否使用磁盘。每个阶段管道限制为 100MB 的内存。如果一个节点管道超过这个极限,MongoDB 将产生一个错误。为了能够在处理大型数据集,可以设置 allowDiskUse 为 true 来在聚合管道节点把数据写入临时文件。这样就可以解决 100MB 的内存的限制。 cursor maxTimeMS bypassDocumentValidation readConcern collation 参考文档https://docs.mongodb.com/manual/reference/method/db.collection.aggregate/#db.collection.aggregatehttp://www.mongodb.org.cn/tutorial/19.html","link":"/blog/46c25054.html"},{"title":"netstat命令详解","text":"功能说明netstat 命令用来打印 Linux 中网络系统的状态信息，它可以用来查询整个 Linux 系统的网络情况，包括 tcp,udp 以及 Unix 套接字；另外它还能列出路由表，接口状态和多播成员等信息。 选项-a 或–all：显示所有连线中的 Socket；-A&lt;网络类型&gt;或–&lt;网络类型&gt;：列出该网络类型连线中的相关地址；-c 或–continuous：持续列出网络状态；-C 或–cache：显示路由器配置的快取信息；-e 或–extend：显示网络其他相关信息；-F 或–fib：显示 FIB；-g 或–groups：显示多重广播功能群组组员名单；-h 或–help：在线帮助；-i 或–interfaces：显示网络界面信息表单；-l 或–listening：显示监控中的服务器的 Socket；-M 或–masquerade：显示伪装的网络连线；-n 或–numeric：直接使用 ip 地址，而不通过域名服务器；-N 或–netlink 或–symbolic：显示网络硬件外围设备的符号连接名称；-o 或–timers：显示计时器；-p 或–programs：显示正在使用 Socket 的程序识别码和程序名称；-r 或–route：显示 Routing Table；-s 或–statistice：显示网络工作信息统计表；-t 或–tcp：显示 TCP 传输协议的连线状况；-u 或–udp：显示 UDP 传输协议的连线状况；-v 或–verbose：显示指令执行过程；-V 或–version：显示版本信息；-w 或–raw：显示 RAW 传输协议的连线状况；-x 或–unix：此参数的效果和指定”-A unix”参数相同；–ip 或–inet：此参数的效果和指定”-A inet”参数相同。 常用命令123456789101112131415161718#列出所有端口（LISTEN，ESTABLISHED）netstat -a #列出所有端口netstat -at #列出所有tcp端口netstat -au #列出所有udp端口#查看程序运行的端口（LISTEN，ESTABLISHED）netstat -apnetstat -ap | grep '程序名'netstat -ap | grep 8080#显示路由表的信息netstat -r#持续输出netstat信息netstat -c #每隔一秒输出网络信息#显示网络接口列表netstat -i","link":"/blog/a568fc3c.html"},{"title":"JavaScript装饰器","text":"随着 ES6 里引入了类，目前 ECMAScript 有一个提案，引入了装饰器（Decorator）函数来标注极或修改类及其成员。装饰器是一种特殊类型的声明，它能够被附加到类声明，方法， 访问符，属性或参数上。 装饰器使用 @expression 这种形式，expression 求值后必须为一个函数，它会在运行时被调用，被装饰的声明信息做为参数传入。 关于装饰器的详细介绍请参阮一峰的《ECMAScript 6 入门》一书 稍微有点遗憾，装饰器并没有被标准化，不过我们可以使用 TypeScript, 或者利用 babel 进行转化 TypeScript 使用装饰器TypeScript 要启用装饰器，你必须在命令行或 tsconfig.json 里启用 experimentalDecorators 编译器选项： 注意: 装饰器是一项实验性特性，在未来的版本中可能会发生改变。 命令行: 1tsc --target ES5 --experimentalDecorators tsconfig.json: 123456{ &quot;compilerOptions&quot;: { &quot;target&quot;: &quot;ES5&quot;, &quot;experimentalDecorators&quot;: true }} 详情请参考：https://www.tslang.cn/docs/handbook/decorators.html babel 转化装饰器装饰器的转化依赖一个核心插件 babel-plugin-transform-decorators-legacy。 12npm i -D babel-core babel-loader babel-preset-es2015npm i -D babel-plugin-transform-decorators-legacy .babelrc 12345678{ &quot;presets&quot;: [ // 把es6转成es5 &quot;babel-preset-es2015&quot; ], // 把装饰器语法转成es5 &quot;plugins&quot;: [&quot;transform-decorators-legacy&quot;]}","link":"/blog/6c496433.html"},{"title":"在koa中使用装饰器","text":"在使用 koa 开发的过程中，经常会忘记把 controller 的方法加到 router 中去，期望使用 decorator 实现路由配置及一些参数校验。 示范代码均采用 TypeScript，实现效果如下： 1234567891011121314151617181920212223242526272829303132333435export default class UserController { /** * 获取用户列表 * * @param {Context} ctx * @returns * @memberof UserController */ @router.get('/user/getUserListByGroup') @validateQuery({ group: Joi.number() .required() .error(new Error('用户组不能为空')), }) async getUserListByGroup(ctx: Context) { return getUserInfoList(ctx.params); } /** * 修改用户信息 * * @param {Context} ctx * @returns * @memberof UserController */ @router.post('/user/setUserInfo') @allow('json') @validateBody({ userId: Joi.number().required(), group: Joi.number().required(), }) async setUserInfo(ctx: Context) { return updateUserInfo({ group: ctx.params.group }, { userId: ctx.params.userId }); }} router 装饰器首先要定义一个 Router 类,使用 routerSet 存储路由信息，在 init 方法加载所有控制器和挂载路由 123456789101112131415161718192021222324252627282930313233343536373839/** * 路由 * * @export * @class Router */export default class Router { // 用于存储路由信息 static routerSet: Set&lt;{ method: string, path: string, middlewares: Koa.Middleware[], }&gt; = new Set(); /** * 初始化路由 * * @static * @returns * @memberof Router */ static init() { // 加载所有控制器 glob.sync(join(__dirname, '../controller/**/*.js')).forEach(require); // 挂载路由 for (const { method, path, middlewares } of this.routerSet) { router[method](path, ...middlewares); } // 404 router.all('*', (ctx: Koa.Context) =&gt; { ctx.status = 404; ctx.error('Router Not Found'); }); return router; }} 实现装饰器,把路由信息和处理函数保存 123456789101112131415161718192021222324252627export function get(path: string) { return addRouterDecorator(path, 'get');}/** * 路由装饰器 * * @param {string} path 路径 * @param {string} method 方法 * @returns */function addRouterDecorator(path: string, method: string) { assert( typeof method === 'string' &amp;&amp; typeof path === 'string', 'method and path should be string', ); return (target: any, name: string, descriptor: PropertyDescriptor) =&gt; { Router.routerSet.add({ method: method, path, middlewares: toArray(Reflect.get(target, name)), }); return descriptor; };} koa 中间件装饰器普通的 koa 中间件装饰器则更为简单，不需额外的存储挂载过程，直接定义就好 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263export function validateQuery(schema) { return middlewareDecorator(ValidateMW(schema, 'query'));}export function validateBody(schema) { return middlewareDecorator(ValidateMW(schema, 'body'));}/** * 数据验证中间件，使用joi * * @param {*} schema joi chema * @param {string} [type='query'] * @returns */function ValidateMW(schema: any, type: string = 'query') { assert(!isEmpty(schema), 'schema is empty'); assert(isObject(schema), 'schema should be object'); return async function(ctx: Context, next: Function) { const { error, value } = Joi.validate(ctx.request[type], schema); if (error) { throw new CWErrors(error.message, errCodeEnum.paramTypeError); } ctx.params = { ...ctx.params, ...value }; return next(); };}/** * Content-Type验证 * * @export * @param {string} contentType Content-Type * @returns */export function allow(...contentTypes: string[]) { assert(contentTypes.length &gt; 0, 'ContentType is empty'); return middlewareDecorator((ctx: Context, next: Function) =&gt; { if (!ctx.is(contentTypes)) { throw new CWErrors('不支持当前表单类型'); } return next(); });}/** * 中间件装饰器 * * @param {Middleware} mw * @returns */function middlewareDecorator(mw: Middleware) { return function(target: any, name: string, descriptor: PropertyDescriptor) { const values = toArray(mw, Reflect.get(target, name)); // 把中间件插入数组开头 Reflect.set(target, name, values); return descriptor; };} 注意：如果同一个方法有多个修饰器，会像剥洋葱一样，先从外到内进入，然后由内向外执行。所以中间件需要添加到数组的开头。 1234567891011121314function dec(id) { console.log('evaluated', id); return (target, property, descriptor) =&gt; console.log('executed', id);}class Example { @dec(1) @dec(2) method() {}}// evaluated 1// evaluated 2// executed 2// executed 1 上面代码中，外层修饰器@dec(1)先进入，但是内层修饰器@dec(2)先执行。 完整代码：https://github.com/zubincheung/koa-ts 参考文档：https://segmentfault.com/a/1190000004357419","link":"/blog/575336e8.html"},{"title":"MySQL批量删除大量数据","text":"公司线上的 MySQL 数据库每半年都需要删除大量的无效数据，只保留部分数据，因为历史遗留问题有几个表需要清理二三亿条数据，少的也有几千万，任务非常艰巨。 以下是实现的几个思路： delete一般情况下删除数据是使用 delete，这个是最普遍但是也是效率最低的一个。直接执行 DELETE FROM tb WHERE status=1 会发现删除失败，因为 lock wait timeout exceed 的错误所涉及的记录数太多，因此我们通过 LIMIT 参数分批删除，比如每 10000 条进行一次删除，那么我们可以利用 MySQL 这样的语句来完成: DELETE FROM tb WHERE status=1 LIMIT 10000; 这样的话需要分批次执行，大量的重复劳动，不推荐。 truncate这个操作会把表中所有的数据给清除掉，清空数据的话效率不错。 由于需要保留部分数据，不适用。 表复制Google 一下发现一个很好的方法： 12345678# 把需要保留的数据转存到另外一张表中INSERT INTO tb_copy SELECT * FROM tb WHERE ... ;# 重命名两张表，间接实现删除数据操作RENAME TABLE tb TO tb_old, tb_copy TO tb;# 删除原来的表DROP TABLE tb_old;","link":"/blog/7cae5094.html"},{"title":"JavaScript变量提升","text":"JavaScript 中，函数及变量的声明都将被提升到函数的最顶部。 1234567891011a = 2;console.log(a); //2var a;foo();function foo() { // 每个作用域都会进行提升操作 console.log(b); // undefined var b = 2;} 以上代码段会被引擎理解为如下形式： 123456789101112function foo() { var b; console.log(b); // undefined b = 2;}var a;a = 2;console.log(a); //2foo(); 函数声明和变量声明都会被提升。但是一个值得注意的细节是函数会首先被提升，然后才是变量 123456789101112foo(); // 3var foo;function foo() { console.log(1);}foo = function() { console.log(2);};function foo() { console.log(3);} var foo 尽管出现在 function foo()… 的声明之前，但它是重复的 var 声明被忽略了，但是出现在后面的函数声明还是可以覆盖前面的，所以输出 3 注意：es6 中 let 和 class 不存在变量提升的现象 1234a = 1; // ReferenceError: a is not definedconsole.log(a);let a; 123const a = new A(); // ReferenceError: A is not definedclass A {}","link":"/blog/172256ca.html"},{"title":"HTTP状态码大全","text":"网上收集的常见HTTP 状态码。 1xx (信息类)表示接收到请求并且继续处理 100 Continue（继续）: 请求者应当继续提出请求。服务器返回此代码表示已收到请求的第一部分，正在等待其余部分。 101 Switching Protocols（切换协议）: 请求者已要求服务器切换协议，服务器已确认并准备切换。 2xx (响应成功)表示成功处理了请求的状态代码。 200 OK（成功）: 请求被成功地完成，所请求的资源发送回客户端 201 Created（已创建）: 请求成功并且服务器创建了新的资源。 202 Accepted（已接受）: 服务器已接受请求，但尚未处理完成。 203 Non-Authoritative Information（非授权信息）: 服务器已成功处理了请求，但返回的信息可能来自另一来源。 204 No Content（无内容）: 服务器成功处理了请求，但没有返回任何内容。 205 Reset Content（重置内容）: 服务器成功处理了请求，但没有返回任何内容，用户代理必须重置当前已经浏览过的文件。 206 Partial Content（部分内容）: 服务器成功处理了部分 GET 请求。 3xx (重定向类)：为了完成指定的动作，必须接受进一步处理 300 Multiple Choices（多种选择）: 针对请求，服务器可执行多种操作。服务器可根据请求者 (user agent) 选择一项操作，或提供操作列表供请求者选择。 301 Moved Permanently（永久移动）: 请求的网页已永久移动到新位置。服务器返回此响应（对 GET 或 HEAD 请求的响应）:时，会自动将请求者转到新位置。 302 Found（临时移动）: 服务器目前从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 303 See Other（查看其他位置）: 请求者应当对不同的位置使用单独的 GET 请求来检索响应时，服务器返回此代码。 304 Not Modified（未修改）: 自从上次请求后，请求的网页未修改过。服务器返回此响应时，不会返回网页内容。 305 Use Proxy（使用代理）: 请求者只能使用代理访问请求的网页。如果服务器返回此响应，还表示请求者应使用代理。 306 （未被使) 在最新版的规范中，此状态码已经不再被使用。 307 Temporary Redirect（临时重定向）: 服务器临时从不同位置的网页响应请求，但请求者应继续使用原有位置来进行以后的请求。 http 重定向 301/302/303/307 4xx（客户端错误类）这些状态代码表示客户端请求可能出错，服务器无法处理。 400 Bad Request（错误请求）： 客户端请求有语法错误，不能被服务器所理解。 401 Unauthorized（未授权）： 请求要求身份验证。 对于需要登录的网页，服务器可能返回此响应。 402 Payment Required（未使用）：是为了将来可能的需求而预留的。 403 Forbidden（禁止）： 服务器拒绝请求。 404 Not Found（未找到）： 服务器找不到请求的资源。 405 Method Not Allowed（方法禁用）： 禁用请求中指定的方法。 406 Not Acceptable（不接受）： 请求的资源的内容特性无法满足请求头中的条件，因而无法生成响应实体。 407 Proxy Authentication Required（需要代理授权）： 此状态代码与 401（未授权）类似，只不过客户端必须在代理服务器上进行身份验证。 408 Request Timeout（请求超时）： 服务器等候请求时发生超时。 409 Conflict（冲突）： 服务器在完成请求时发生冲突。服务器必须在响应中包含有关冲突的信息。 410 Gone（已删除）： 如果请求的资源已永久删除，服务器就会返回此响应。 411 Length Required（需要有效长度）： 服务器不接受不含有效内容长度标头字段的请求。 412 Precondition Failed（未满足前提条件）： 服务器未满足请求者在请求中设置的其中一个前提条件。 413 Payload Too Large（请求实体过大）： 服务器无法处理请求，因为请求实体过大，超出服务器的处理能力。 414 URI Too Long（请求的 URI 过长）： 请求的 URI（通常为网址）过长，服务器无法处理。 415 Unsupported Media Type（不支持的媒体类型）： 请求的格式不受请求页面的支持。 416 Range Not Satisfiable（请求范围不符合要求）： 如果页面无法提供请求的范围，则服务器会返回此状态代码。 417 Expectation Failed（未满足期望值）： 在请求头 Expect 中指定的预期内容无法被服务器满足。 5xx（服务器错误）这些状态代码表示服务器在尝试处理请求时发生内部错误。 这些错误可能是服务器本身的错误，而不是请求出错。 500 Internal Server Error（服务器内部错误）： 服务器遇到错误，无法完成请求。 501 Not Implemented（尚未实施）： 服务器不具备完成请求的功能。例如，服务器无法识别请求方法时可能会返回此代码。 502 Bad Gateway（错误网关）： 服务器作为网关或代理，从上游服务器收到无效响应。 503 Service Unavailable（服务不可用）： 服务器目前无法使用（由于超载或停机维护）。通常，这只是暂时状态。 504 Gateway Timeout（网关超时）： 服务器作为网关或代理，但是没有及时从上游服务器收到请求。 505 HTTP Version Not Supported（HTTP 版本不受支持）： 服务器不支持请求中所用的 HTTP 协议版本。","link":"/blog/10dc9444.html"},{"title":"ES6中Map和Object的区别","text":"JavaScript 的对象（Object）和 Map，本质上是键值对的集合（Hash 结构） ES6 中 Map 相对于 Object 对象有几个区别： 对象（Object）和只能用字符串和 Symbol 当作键，Map 的 key 可以是任何基本类型或者对象 Map 可以通过 size 获取到长度，Object 获取长度的比较复杂 Map 具有 Symbol.iterator 属性,可通过 for…of 循环遍历 Map 转为对象： 如果所有 Map 的键都是字符串或者 Symbol，它可以无损地转为对象。 如果有非字符串的键名，那么这个键名会被转成字符串，再作为对象的键名。","link":"/blog/a807d937.html"},{"title":"egg中egg-sequelize和egg-mongoose不能同时使用","text":"今天在项目中加入 egg-mongoose 插件，配置好 config 文件和 plugin.ts 信息后，运行代码，在加载 mongoose 插件的时候报错了，错误信息： 1nodejs.TypeError: Cannot assign to read only property 'model' of object '#&lt;Application&gt;' 回看下代码发现原有的 egg-sequelize 已经挂载 model 到 app 上面 去掉其中一个插件都可以正常运行,看来 egg-sequelize 和 egg-mongoose 插件冲突了，excuse me? 错误原因浏览 egg-sequelize 源码发现已经定义好 model 不能被重写 12345Object.defineProperty(model, delegate[len - 1], { value: sequelize, writable: false, configurable: true,}); egg-mongoose 挂载 model 到 app 上时就会报Cannot assign to read only property错误了。 解决方案 给官方提 issue：把问题和解决方案描述一下； 降级方案：去掉 MongoDB 的 model 层使用其他插件如 egg-mongo-native 或者自己封装一个； model 层使用一种数据库,将使用 MongoDB 和 MySQL 的业务拆分为两个服务；","link":"/blog/58380429.html"},{"title":"JavaScript之常量","text":"简单类型常量众所周知 ES6 新增的 const 关键字可以用来声明常量。常量是块级作用域，很像使用 let 语句定义的变量。常量的值不能通过重新赋值来改变，并且不能重新声明。 12const name = 'zubin';name = 'zhang'; //TypeError: Assignment to constant variable. const 实际上保证的，并不是变量的值不得改动，而是变量指向的那个内存地址所保存的数据不得改动，对于基本数据类型（Number、String、Boolean 等），值就保存在变量指向的内存地址，因此等同于常量。对于复合类型（Object、Array 等）变量指向的内存地址，保存的只是一个指向实际数据的指针，const 只能保证这个指针是固定的（即总是指向另一个固定的地址），至于它指向的数据结构是不是可变的，就完全不能控制了。 12345678910111213const foo = {};// 为 foo 添加一个属性，可以成功foo.prop = 123;foo.prop; // 123// 将 foo 指向另一个对象，就会报错foo = {}; // TypeErrorconst a = [];a.push('Hello'); // 可执行a.length = 0; // 可执行a = ['Dave']; // TypeError 复合类型常量如果真的想将对象冻结，应该使用 Object.freeze 方法，冻结指的是不能向这个对象添加新的属性，不能修改其已有属性的值，不能删除已有属性，以及不能修改该对象已有属性的可枚举性、可配置性、可写性。该方法返回被冻结的对象。 12345const foo = Object.freeze({});// 常规模式时，下面一行不起作用；// 严格模式时，该行会报错foo.prop = 123; 除了将对象本身冻结，对象的属性也应该冻结。 12345678var constantize = obj =&gt; { Object.freeze(obj); Object.keys(obj).forEach((key, i) =&gt; { if (typeof obj[key] === 'object') { constantize(obj[key]); } });}; 参考文档阮一峰：ECMAScript 6 入门MDN：Object.freeze","link":"/blog/a17a06f.html"},{"title":"JavaScript深入之引用类型","text":"基本类型JavaScript 变量可以用来保存两种类型的值：基本类型值和引用类型值。基本类型值指的是 简单的数据段，包括以下 6 种基本数据类型：Undefined、Null、Boolean、Number 、 String 和 Symbol。基本类型保存在栈中，存储的是具体的值，是轻量级的数据存储方式。 引用类型引用类型的值是保存在内存中的对象。与其他语言不同，JavaScript 不允许直接访问内存中的位置， 也就是说不能直接操作对象的内存空间。在操作对象时，实际上是在操作对象的引用而不是实际的对象。 为此，引用类型的值是按引用访问的。 Object 类型Object 是一个基础类型，其他所有类型都从 Object 继承了基本的行为。创建 Object 实例的方式有两种。第一种是使用 new 操作符后跟 Object 构造函数，另一种方式是使用对象字面量表示法。 12345678910// new操作符创建对象let person1 = new Object();person.name = 'Zubin';person.age = 18;// 字面量表示法let person2 = { name: 'Zubin', age: 18,}; Array 对象除了 Object 之外，Array 类型恐怕是 ECMAScript 中最常用的类型了。而且，ECMAScript 中 的数组与其他多数语言中的数组有着相当大的区别。 ECMAScript 数组的每一项可以保存任何类型的数据。 ECMAScript 数组的大小是可以动态调整的，即可以随着数据的添加自动增长以容 纳新增数据。 方法： Array.from()从类数组对象或者可迭代对象中创建一个新的数组实例。 Array.isArray()用来判断某个变量是否是一个数组对象。 Array.of()根据一组参数来创建新的数组实例，支持任意的参数数量和类型。 数组实例的常用方法： 方法名称 说明 concat 用于连接两个或更多的数组并返回结果,arr1.concat(arr2) join 把数组的所有元素放入一个字符串，元素通过制定的分隔符进行分离 arr1.join(‘,’) pop 删除并返回数组中的最后一个元素 arr1.pop() push 向数组的末尾添加一个或更多元素，并返回新的长度 arr1.push(1) reverse 颠倒数组中的元素顺序,arr1.reverse() shift 删除并返回数组中的第一个元素 arr1.shift() slice 从某个已有的数组返回指定的元素 sort 对数组的元素进行排序 arr1.sort() splice 删除元素，并向数组中添加新元素 toString 把数组转成字符串 arr1.toString() toLocaleString 把数组转换为本地字符串 arr1.toLocaleString() valueOf 返回数组对象的原始值 Date 对象创建 Date 实例用来处理日期和时间。Date 对象基于 1970 年 1 月 1 日（世界标准时间）起的毫秒数。 12345678var today = new Date();var today = new Date(1453094034000); // by timestamp(accurate to the milliseconds)var birthday = new Date('December 17, 1995 03:24:00');var birthday = new Date('1995-12-17T03:24:00');var birthday = new Date(1995, 11, 17);var birthday = new Date(1995, 11, 17, 3, 24, 0);var unixTimestamp = Date.now(); // in milliseconds 方法 Date.now()返回自 1970-1-1 00:00:00 UTC (世界标准时间)至今所经过的毫秒数。 Date.parse()解析一个表示日期的字符串，并返回从 1970-1-1 00:00:00 所经过的毫秒数。 Date.UTC()接受和构造函数最长形式的参数相同的参数（从 2 到 7），并返回从 1970-01-01 00:00:00 UTC 开始所经过的毫秒数。 RegExp 类型类型是 ECMAScript 支持正则表达式的一个接口，提供了最基本的和一些高级的正则表 达式功能。 1234567891011var regex1 = /\\w+/;var regex2 = new RegExp('\\\\w+');console.log(regex1);// expected output: /\\w+/console.log(regex2);// expected output: /\\w+/console.log(regex1 === regex2);// expected output: false Function 类型Function 构造函数 创建一个新的 Function 对象。 在 JavaScript 中, 每个函数实际上都是一个 Function 对象。 1234var sum = new Function('a', 'b', 'return a + b');console.log(sum(2, 6));// expected output: 8 参考文档MDN JavaScriptJavaScript 高级程序设计（第 3 版）","link":"/blog/e4079434.html"},{"title":"JavaScript深入之数据类型","text":"一、内置类型最新的 ECMAScript 标准定义了 7 种数据类型: 空值（null） 未定义（undefined） 布尔值（boolean） 数字（number） 字符串（string） 符号（symbol，ES6 中新增） 对象（object） 除对象为“复杂类型”之外，其他统称为“基本类型”。 二、typeof 操作符JavaScript 是一种弱类型或者说动态语言。这意味着你不用提前声明变量的类型，在程序运行过程中，类型会被自动确定。这也意味着你可以使用同一个变量保存不同类型的数据,我们可以通过 typeof 操作符来判断变量的类型。 1234567891011typeof undefined; // &quot;undefined&quot;typeof true; // &quot;boolean&quot;typeof 42; // &quot;number&quot;;typeof '42'; // &quot;string&quot;;typeof Symbol(); // &quot;symbol&quot;typeof null; // &quot;object&quot;?typeof { life: 42 }; // &quot;object&quot;typeof [1, 2, 3]; // &quot;object&quot;typeof function foo() {}; // &quot;function&quot; null 比较 特殊 ， typeof 对它的处理有问题,正确的返回结果应该是 “null” ， 但这个 bug 由来已久， 在 JavaScript 中已经存在了将近 二十年，也许永远也不会修复，因为这牵涉到太多的 Web 系统，“修复”它会 产生 更多的 bug， 令许多系统无法正常工作。V8 曾经修正并实现过 typeof null === ‘null’,但最终证明不可行。http://wiki.ecmascript.org/doku.php?id=harmony:typeof_null 查阅 ECMAScript 规范就会知道， Object 类型包含 Function、Array、Date、RegExp 等。 三、原始值( primitive values )Undefined 类型一个没有被赋值的变量会有个默认值 undefined Null 类型Null 类型只有一个值 null。从逻辑角度来看，null 值表 示一个空对象指针，而这也正是使用 typeof 操作符检测 null 值时会返回”object”的原因 布尔类型布尔表示一个逻辑实体，可以有两个值：true 和 false，要将一个值转换为其对应的 Boolean 值，可以调用转型函数 Boolean() 数据类型 转换为 true 的值 转换成 false 的值 string 任何非空字符串 “”(空字符串) number 任何非零数值（包括无穷大、无穷小） 0 和 NaN object 任何对象 null undefined undefined symbol 任何符号 数字类型根据 ECMAScript 标准，JavaScript 中只有一种数字类型：基于 IEEE 754 标准的双精度 64 位二进制格式的值（-(2^63 -1) 到 2^63 -1）。它并没有为整数给出一种特定的类型。除了能够表示浮点数外，还有一些带符号的值：+Infinity(正无穷)，-Infinity(负无穷) 和 NaN (非数值，Not-a-Number)。 要检查值是否大于或小于 +/-Infinity，你可以使用常量 Number.MAX_VALUE 和 Number.MIN_VALUE。在 ECMAScript 6 中，你也可以通过 Number.isSafeInteger() 方法还有 Number.MAX_SAFE_INTEGER 和 Number.MIN_SAFE_INTEGER 来检查值是否在双精度浮点数的取值范围内。 字符串类型节JavaScript 的字符串类型用于表示文本数据。它是一组 16 位的无符号整数值的“元素”。在字符串中的每个元素占据了字符串的位置。第一个元素的索引为 0，下一个是索引 1，依此类推。字符串的长度是它的元素的数量。 不同于类 C 语言，JavaScript 字符串是不可更改的。这意味着字符串一旦被创建，就不能被修改。但是，可以基于对原始字符串的操作来创建新的字符串。例如： 获取一个字符串的子串可通过选择个别字母或者使用 String.substr(). 两个字符串的连接使用连接操作符 (+) 或者 String.concat(). 注意代码中的“字符串类型”！可以使用字符串来表达复杂的数据。以下是一些很好的性质： 容易连接构造复杂的字串符 字符串容易被调试(你看到的往往在字符串里) 字符串通常是许多 APIs 的常见标准 (input fields, local storage values, XMLHttpRequest 当使用 responseText 等的时候回应) 而且他只能与字符串一同使用。 按照惯例, 字符串一般可以用来表达任何数据结构。这不是一个好主意。例如，使用一个分隔符，一个可以模仿一个列表(一个 JavaScript 的数组可能更适合一些) 。不幸的是，当一个分隔符在用于列表中的元素时，打乱了这个列表。 一个转义字符等。所有这些惯例都变成了一个不存在的维护负担而没有正确的工具使用。 表达文本数据和符号数据时候推荐使用字符串。当表达复杂的数据时，使用字符串解析和适当的缩写。 符号类型符号(Symbols)是 ECMAScript 第 6 版新定义的。符号类型是唯一的并且是不可修改的, 并且也可以用来作为 Object 的 key 的值。 参考文档JavaScript 高级程序设计（第 3 版）你不知道的 JavaScript（中卷）JavaScript 数据类型和数据结构","link":"/blog/d063347b.html"},{"title":"JavaScript深入之原生函数","text":"JavaScript 为基本数据类型值提供了封装对象，称为原生函数，常见的原生函数包括 String() Number() Boolean() Array() Object() Function() RegExp() Date() Error() Symbol() 它们可以被当作构造函数来使用， 但其构造出来的对象可能会和我们设想的有所 出入 1234let a = new String('abc');typeof a; // 是&quot;object&quot;，不是&quot;String&quot;a instanceof String; // trueObject.prototype.toString.call(a); // &quot;[object String]&quot; 通过构造函数（如 new String(“abc”) ）创建出来的是封装了基本类型值（如 “abc” ）的封 装对象。 内部属性[[Class]]所有 typeof 返回值为 “object” 的对象（如数组）都包含一个内部属性 [[Class]] （我们可 以把它看作一个内部的分类，而非传统的面向对象意义上的类）。这个属性无法直接访问， 一般通过 Object.prototype.toString(..) 来查看。例如： 123456789Object.prototype.toString.call([1, 2, 3]); // &quot;[object Array]&quot;Object.prototype.toString.call(/regex-literal/i); // &quot;[object RegExp]&quot;Object.prototype.toString.call(null); // &quot;[object Null]&quot;Object.prototype.toString.call(undefined); // &quot;[object Undefined]&quot;Object.prototype.toString.call('abc'); // &quot;[object String]&quot;Object.prototype.toString.call(42); // &quot;[object Number]&quot;Object.prototype.toString.call(true); // &quot;[object Boolean]&quot; 内部属性和创建该对象的内建原生构造函数相对应,虽然 Null() 和 Undefined() 这样的原生构造函数并不存在，但是内部属性值仍 然是 “Null” 和 “Undefined” 。其他基本类型值（如字符串、数字和布尔）的情况有所不同，它们的值被各自的封装对象自动包装，所以它们的内部属性值分别为 “String” 、 “Number” 和 “Boolean” 。 封装原生函数为基本数据类型值提供了该子类型所特有的方法和属性（如：String#trim() 和 Array#concat(..)）。 对于简单标量基本类型值，比如 “abc” ，如果要访问它的 length 属性或 String.prototype 方法， JavaScript 引擎会自动对该值进行封装（即用相应类型的封装对象来包装它）来实现对这些属性和方法的访问。 123456789var a = 'abc';a.length; // 3a.toUpperCase(); // &quot;ABC&quot;var b = new Boolean(false);if (!b) { console.log('Oops'); // 执行不到这里} 拆封如果想要得到封装对象中的基本类型值，可以使用 valueOf() 函数： 1234567var a = new String('abc');var b = new Number(42);var c = new Boolean(true);a.valueOf(); // &quot;abc&quot;b.valueOf(); // 42c.valueOf(); // true 参考文档你不知道的 JavaScript（中卷）","link":"/blog/75eb6888.html"},{"title":"JavaScript深入之类型转换","text":"JavaScript 是弱类型的语言，它的取值非常灵活。你期望一种类型值的时候，你可以提供任何类型的值，JavaScript 将根据需要自行转换类型，在 JavaScript 中通常将它们统称为 强制类型转换。 ECMAScript 规范中Type Conversion 章节中定义了转换规则。这里我们着重介绍 **ToPrimitive、ToBoolean**、 ToNumber 和 ToString 。 ToPrimitive在 JavaScript 中，想要将对象转换成原始值，必然会调用 toPrimitive()内部函数，那么它是如何工作的呢？ 12ToPrimitive(input [, PreferredType])input 是输入的值，preferedType 是期望转换的类型， 输入类型 结果 Undefined 返回 input 自身 Null 返回 input 自身 Boolean 返回 input 自身 Number 返回 input 自身 String 返回 input 自身 Symbol 返回 input 自身 Object 返回该对象的默认值。具体过程请（通过内部操作 DefaultValue ，参见 ES5 规范 8.12.8 节） 如果 PreferredType 是 Number，执行顺序如下： 如果 input 为 primitive，返回。 否则，input 为 Object。调用 obj.valueOf()。如果结果是 primitive，返回。 否则，调用 obj.toString(). 如果结果是 primitive，返回。 如果 valueOf() 和 toString() 均不返回基本类型值，会产生 TypeError 错误。 如果 PreferredType 是 String，步骤 2 跟 3 互换，如果 PreferredType 为空，Date 类型默认为 String，其他都是 Number。 ToBooleanToBoolean 运算符根据下表将其参数转换为布尔值类型的值： 输入类型 结果 Undefined false Null false Boolean 结果等于输入的参数（不转换）。 Number 如果参数是 +0, -0, 或 NaN，结果为 false ；否则结果为 true。 String 如果参数参数是空字符串（其长度为零），结果为 false，否则结果为 true。 Object true Symbol true ToNumberToNumber 运算符根据下表将其参数转换为数值类型的值： 输入类型 结果 Undefined NaN Null +0 Boolean 如果参数是 true，结果为 1。如果参数是 false，此结果为 +0。 Number 直接返回。 String 参见下文的文法和注释。 Symbol TypeError Object 执行下列步骤： 1. 设 primValue 为 ToPrimitive( 输入参数 , hint Number)。2.返回 ToNumber(primValue)。 ToStringToString 运算符根据下表将其参数转换为字符串类型的值： 输入类型 结果 Undefined “undefined” Null “null” Boolean 如果参数是 true，那么结果为 “true”。如果参数是 false，那么结果为 “false”。 Number 详见7.1.12.1NumberToString String 直接返回 。 Symbol TypeError Object 执行下列步骤：1、设 primValue 为 ToPrimitive( 输入参数 , hint String)。2、返回 ToString(设 primValue 为 )。 参考资料Type Conversion and TestingType Conversion","link":"/blog/b2c34d5.html"},{"title":"时区问题小结","text":"修改基于 Alpine 的 Docker 容器的时区在容器中修改进入容器 1＃ docker exec -it container_name /bin/sh 安装 timezone,列出安装的时区文件，验证是否下载成功。 12＃ apk add -U tzdata＃ ls /usr/share/zoneinfo 拷贝需要的时区文件到 localtime，国内需要的是 Asia/Shanghai： 1＃ cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 验证时区 12＃ dateWed Dec 12 19:09:09 CST 2018 CST 即为 中国标准时间。 移除时区文件： 1＃ apk del tzdata 在 Dockerfile 指定时区1234567# Install base packages, set timezoneRUN apk update &amp;&amp; apk add curl bash tree tzdata# cp -r -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeENV TZ Asia/Shanghai Node.Js 中 sequelize 时区的配置方法sequelize 默认情况下，保存日期时会转换成 +00:00 时区 解决方式：sequelize 时配置时区timezone: ‘+08:00’ 1234567891011const sequelize = new Sequelize(config.database, config.username, config.password, { host: config.host, port: config.port, dialect: 'mysql', pool: { max: 5, min: 0, idle: 10000, }, timezone: '+08:00',}); 参考文档Setting the timezoneSequelize.html#instance-constructor-constructor","link":"/blog/undefined.html"},{"title":"CPU使用率","text":"什么是 CPU 使用率CPU 使用率就是除了空闲时间外的其他时间占总 CPU 时间的百分比 事实上，为了计算 CPU 使用率，性能工具一般都会取间隔一段时间（比如 3 秒）的两次值，作差后，再计算出这段时间内的平均 CPU 使用率，即 需要注意的是，**&gt;性能分析工具给出的都是间隔一段时间的平均 CPU 使用率，所以要注意间隔时间的设置**，特别是用多个工具对比分析时，你一定要保证它们用的是相同的间隔时间。 怎么查看 CPU 使用率 top 显示了系统总体的 CPU 和内存使用情况，以及各个进程的资源使用情况。 系统的 CPU 使用率(%Cpu) pidstat 专门分析每个进程 CPU 使用情况的工具 用户态 CPU 使用率 （%usr）； 内核态 CPU 使用率（%system）； 运行虚拟机 CPU 使用率（%guest）； 等待 CPU 使用率（%wait）； 以及总的 CPU 使用率（%CPU）。 perf 分析 CPU 性能问题 pstree 用树状形式显示所有进程之间的关系,可以用来查找一个进程的父进程 execsnoop 专为短时进程设计的工具 CPU 使用率过高怎么办？ CPU 使用率是最直观和最常用的系统性能指标，更是我们在排查性能问题时，通常会关注的第一个指标。所以我们更要熟悉它的含义，尤其要弄清楚用户（%user）、Nice（%nice）、系统（%system） 、等待 I/O（%iowait） 、中断（%irq）以及软中断（%softirq）这几种不同 CPU 的使用率。比如说： 用户 CPU 和 Nice CPU 高，说明用户态进程占用了较多的 CPU，所以应该着重排查进程的性能问题。 系统 CPU 高，说明内核态占用了较多的 CPU，所以应该着重排查内核线程或者系统调用的性能问题。 I/O 等待 CPU 高，说明等待 I/O 的时间比较长，所以应该着重排查系统存储是不是出现了 I/O 问题。 软中断和硬中断高，说明软中断或硬中断的处理程序占用了较多的 CPU，所以应该着重排查内核中的中断服务程序。 碰到 CPU 使用率升高的问题，你可以借助 top、pidstat 等工具，确认引发 CPU 性能问题的来源；再使用 perf 等工具，排查出引起性能问题的具体函数。 碰到常规问题无法解释的 CPU 使用率情况时，首先要想到有可能是短时应用导致的问题，比如有可能是下面这两种情况。 应用里直接调用了其他二进制程序，这些程序通常运行时间比较短，通过 top 等工具也不容易发现 应用本身在不停地崩溃重启，而启动过程的资源初始化，很可能会占用相当多的 CPU","link":"/blog/3ad6e1fc.html"},{"title":"CPU上下文切换","text":"什么是 CPU 上下文切换CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境，因此也被叫做CPU 上下文。 CPU 上下文切换，就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。 进程上下文切换 Linux 按照特权等级，把进程的运行空间分为内核空间和用户空间。 内核空间（Ring 0）具有最高权限，可以直接访问所有资源。 用户空间（Ring 3）只能访问受限资源，不能直接访问内存等硬件设备，必须通过系统调用陷入到内核中，才能访问这些特权资源。 系统调用(特权模式切换):一个进程用户态与内核态的互相转变 上下文切换:从一个进程切换到另一个进程运行 虚拟内存、栈、全局变量等用户空间的资源 内核堆栈、寄存器等内核空间的状态 一次系统调用的过程，发生了两次 CPU 上下文切换。 什么时候会发生？ 进程 CPU 时间片耗尽，被系统挂起，切换到其他正在等待 CPU 的进程 系统资源不足时进程被系统挂起，系统调度其他进程运行 进程通过睡眠函数 sleep 这样的方法将自己主动挂起 有优先级更高的进程运行，当前程序会被挂起 发生硬件中断，转而执行内核中的终端服务程序 线程上下文切换线程与进程的区别 线程是调度的基本单位，而进程是资源拥有的基本单位 当进程只有一个线程时，可以认为进程就等于线程 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源，在上下文切换时，这些资源不需要修改 线程有自己的私有数据，例如栈和寄存器等，在上下文切换时需要保存 什么时候会发生 前后两个线程属于不同进程。此时因为资源不共享，因此等同于进程上下文切换 前后两个线程属于同一个进程，因为虚拟内存共享，所以只需要切换私有数据、寄存器等不共享的数据 虽然同为上下文切换，但同进程内的线程切换，要比多进程间的切换消耗更少的资源，而这，也正是多线程代替多进程的一个优势。 中断上下文切换 中断处理会打断进程的正常调度和执行 对同一个 CPU 来说，中断处理比进程拥有更高的优先级 怎么查看系统的上下文切换情况vmstatvmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。 需要特别关注的四列内容： cs(context switch) 表示每秒上下文切换的次数 in(interrupt)表示每秒中断次数 r(Running or Runnable)表示就绪队列的长度，也就是正在运行和等待 CPU 的进程数 b(Blocked)表示处于不可中断睡眠状态的进程数 #每隔 5 秒输出一组数据 pidstatvmstat 只给出了系统总体的上下文切换情况，要想查看每个进程的详细情况，就需要使用 pidstat 了。给它加上 -w 选项，你就可以查看每个进程上下文切换的情况了。 需要特别关注的两列内容 cswch 表示每秒自愿上下文切换的次数 nvcswch 表示每秒非自愿上下文切换的次数 自愿上下文切换：进程无法获取所需资源 非自愿上下文切换：进程由于时间片已到等原因，被系统强制调度 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 IO 等其他问题 非自愿上下文切换变多了，说明进程都在被强制调度，即在争抢 CPU，说明 CPU 成为瓶颈 中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看/proc/interrupts 文件来分析具体的中断类型 小结：不管是哪种场景导致的上下文切换，我们应该知道： CPU 上下文切换，是保证 Linux 系统正常工作的核心功能之一，一般情况下不需要我们特别关注。 但过多的上下文切换，会把 CPU 时间消耗在寄存器、内核栈以及虚拟内存等数据的保存和恢复上，从而缩短进程真正运行的时间，导致系统的整体性能大幅下降。 碰到上下文切换次数过多的问题时，我们可以借助 vmstat 、 pidstat 和 /proc/interrupts 等工具，来辅助排查性能问题的根源。","link":"/blog/21989671.html"},{"title":"日志框架winston的使用","text":"日志对于问题定位、调试，系统性能调优至关重要，尤其是系统复杂以及在线运行的情况下。之前的项目日志输出一直用 log4js,输出到一个文件，最近对那一块进行重构。 分别考虑了两款 Node.js 框架，分别是Bunyan 和 Winston。 Winston 是 Node.js 最流行的日志框架之一，设计为一个简单通用的日志库，支持多传输 Bunyan 以略微不同的方式处理结构化，机器可读性被重点对待。实际上就是 JSON.stringify 的一个输出。 预期效果 日志分级 根据不同的代码分层来产生不同的 log 输出到不同的文件。 输出到 log 文件同时还可以选择输出到标准输出。 自定义格式化日志输出，输出到标准输出的格式便于读取，输出到 log 文件的格式便于分析。 按天自动切割日志文件。 为何选择 winston github start 数多。 更加灵活，可以灵活的组织 transport，来完成比较复杂的日志输出任务。 日志格式为 json 字符串，方便后期分析，当然可以自定义 format. 支持简单的 log 分析，Profiling。 支持 stream Api。 简单 Log Query Api，当然无法和专业的日志分析工具比。 使用方法定义标准输出 Transport123456789101112131415161718class ConsoleTransport extends winston.transports.Console { constructor(options) { super(options); this.format = winston.format.combine( winston.format(info =&gt; { info.hostname = hostname(); info.pid = process.pid; info.level = info.level.toUpperCase(); return info; })(), winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss,SSSS' }), winston.format.ms(), winston.format.colorize(), winston.format.printf(options.formatter), ); }} 定义文件输出12345678910111213141516171819202122232425262728293031323334353637const winston = require('winston');require('winston-daily-rotate-file');class FileTransport extends winston.transports.DailyRotateFile { constructor(options) { super(options); this.datePattern = 'YYYY-MM-DD'; this.zippedArchive = true; this.maxSize = '100m'; this.maxFiles = '14d'; const defaultFormatter = winston.format.combine( winston.format(info =&gt; { info.hostname = hostname(); info.pid = process.pid; info.level = info.level.toUpperCase(); return info; })(), winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss,SSSS' }), winston.format.ms(), ); if (options.json) { // 输出json格式 this.format = winston.format.combine( defaultFormatter, winston.format.json(options.formatter), ); } else { this.format = winston.format.combine( defaultFormatter, winston.format.printf(options.formatter), ); } }} 调用方法12345678910111213141516171819202122232425262728293031323334353637383940const options = { name: 'app', module: 'app', filePath: './logs', formatter: meta =&gt; { return `${meta.timestamp} ${meta.level} ${meta.hostname} ${meta.pid} (${meta.ms}) [${ meta.module }] ${meta.message}`; }, consoleLevel: 'error', json: false,};const consoleTransport = new ConsoleTransport({ level: options.consoleLevel, name: options.name, formatter: options.consoleFormatter,});const infoTransport = new FileTransport({ level: 'info', name: options.name, filename: join(options.filePath, `info/${options.name}-info-%DATE%.log`), formatter: options.formatter, json: options.json,});const errorTransport = new FileTransport({ level: 'error', name: options.name, filename: join(options.filePath, 'error/error-%DATE%.log'), formatter: options.formatter, json: options.json,});const transports = [consoleTransport, infoTransport, errorTransport];const logger = winston.createLogger({ transports });logger.info('log1');logger.error('error 1'); 代码地址为：https://github.com/zubincheung/cw-logger-winston 输出效果12345678910112018-12-30 10:07:28,4605 INFO zubin-pc.local 59468 (+0ms) [app] app log 12018-12-30 10:07:28,4695 ERROR zubin-pc.local 59468 (+1ms) [app] Error: app error 1Error: app error 1 at Object.it (/Users/zubincheung/ciwong/cw-logger-winston/test/cw-logger.test.js:61:35) at Object.asyncJestTest (/Users/zubincheung/ciwong/cw-logger-winston/node_modules/jest-jasmine2/build/jasmine_async.js:108:37) at resolve (/Users/zubincheung/ciwong/cw-logger-winston/node_modules/jest-jasmine2/build/queue_runner.js:56:12) at new Promise (&lt;anonymous&gt;) at mapper (/Users/zubincheung/ciwong/cw-logger-winston/node_modules/jest-jasmine2/build/queue_runner.js:43:19) at promise.then (/Users/zubincheung/ciwong/cw-logger-winston/node_modules/jest-jasmine2/build/queue_runner.js:87:41) at process.internalTickCallback (internal/process/next_tick.js:77:7)","link":"/blog/76482d89.html"},{"title":"安装指定版本node","text":"mac 环境下,使用 homebrew 安装的 node, 默认是最高版本,如何安装指定版本的 node 呢? 如果之前使用 brew install node 安装过 node,需要先执行brew unlink node 来解绑node。 查找可用的 node 版本 brew search node。 安装你需要的版本, 比如 brew install node@10。 然后 brew link node@10, 这一步可能会报错, 按照提示执行命令就 ok 了, 比如我最后执行的是 brew link --overwrite --force node@10。 node -v 不出意外, 就安装好了你想要的 node 版本。 参考文档: https://www.jianshu.com/p/c5c298486dbd","link":"/blog/42719bd.html"},{"title":"JavaScript数组随机取一部分不重复元素","text":"从一个 JavaScript 数组当中，随机抽取部分元素，构成新数组，要求这些元素不能重复，即随机获取不重复的数组元素。这个问题很简单，相信很多人都会在几分钟内给出以下代码： 12345678910111213141516function randomMembers1(arr, limit) { const result = []; for (let i = 0; i &lt; limit; i++) { result[i] = arr[Math.floor(Math.random() * arr.length)]; for (let j = 0; j &lt; i; j++) { if (result[j] === result[i]) { i--; break; } } } return result;}randomMembers1([11, 12, 13, 14, 15, 16, 17, 18], 5); //[ 18, 16, 12, 17, 14 ] 解决思路就是从第二次随机抽取的元素开始，将抽取的元素与已抽取元素相比较，如果相同，则重新抽取，并再次执行比较的操作。但是这种写法存在循环语句和条件语句多层嵌套，复杂度较高，执行效率很低。随着元素的抽取越多，要比较的次数越来越多，“失败的抽取”概率越来越大。 我们可以优化一下比较的逻辑，比如以下代码： 1234567891011121314151617function randomMembers2(arr, limit) { const hash = {}; const result = []; while (limit &gt; 0) { const index = Math.floor(Math.random() * arr.length); if (!hash[index]) { hash[index] = true; result.push(arr[index]); limit--; } } return result;}randomMembers2([11, 12, 13, 14, 15, 16, 17, 18], 5); //[ 16, 17, 13, 11, 18 ] 和第一种方法相比，节省了第一种方法中依次比较的步骤，但依旧存在“失败抽取”的现象，而且失败抽取的概率没有发生任何变化。 是否可以把抽取到的元素从数组中删除，从而避免重复抽取呢？可以利用 splice 方法，将抽取到的元素从数组当中删除掉，并把返回值存储（push）到结果数组当中。 12345678910111213function randomMembers3(arr, limit) { const result = []; let num = arr.length &gt; limit ? limit : arr.length; while (num &gt; 0) { const index = Math.floor(Math.random() * arr.length); result.push(arr.splice(index, 1)[0]); num--; } return result;} 问题就是这种方法会修改源数组，产生副作用,抽取的元素会从数组中删除。我们可以新建一个数组保存原来数组的下标，从下标数组中进行抽取。 12345678910111213141516function randomMembers4(arr, limit) { const result = []; const keyList = [...arr.keys()]; let num = arr.length &gt; limit ? limit : arr.length; while (num &gt; 0) { const index = Math.floor(Math.random() * keyList.length); const key = keyList.splice(index, 1)[0]; result.push(arr[key]); num--; } return result;} 完整代码","link":"/blog/8e641c9d.html"},{"title":"网络性能指标","text":"从网上收集的一些评估网络性能的指标。 带宽带宽，表示链路的最大传输速率，单位是 b/s（比特 / 秒）。在你为服务器选购网卡时，带宽就是最核心的参考指标。常用的带宽有 1000M、10G、40G、100G 等。 吞吐量吞吐量，表示没有丢包时的最大数据传输速率，单位通常为 b/s （比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽的限制，吞吐量 / 带宽也就是该网络链路的使用率。 延时延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。这个指标在不同场景中可能会有不同的含义。它可以表示建立连接需要的时间（比如 TCP 握手延时），或者一个数据包往返所需时间（比如 RTT）。 PPSPPS，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，而基于 Linux 服务器的转发，很容易受到网络包大小的影响（交换机通常不会受到太大影响，即交换机可以线性转发）。 网络可用性网络可供用户使用的时间百分比。即网络稳定不出故障的时间 / 用户总的使用时间 并发连接数是客户端向服务器发起请求，并建立了 TCP 连接，每秒钟服务器链接的总 TCP 数量 丢包率测试中所丢失数据包数量占所发送数据组的比率。 重传率重新发送信息的与全部的调用信息之间的比值。 响应时间(RT)响应时间是指系统对请求作出响应的时间。 吞吐量(Throughput)吞吐量是指系统在单位时间内处理请求的数量。","link":"/blog/6115ad2.html"},{"title":"JavaScript实现四则混合运算","text":"背景最近在项目中需要自己解析四则混合运算，如果只是简单的加减乘除运算，我相信对大家来说没有任何困难，但是实现带括号的四则运算，还是有一定的难度的。 操作数：小数、整数 运算符：加、减、乘、除 分界符：圆括号 ( ) ， 用于指示运算的先后顺序 这里使用逆波兰表达式解决数值运算以及括号带来的优先级提升问题。 逆波兰表达式 中缀表达式(Infix Notation)是一个通用的算术或逻辑公式表示方法， 操作符是以中缀形式处于操作数的中间。比如1 + 2 + 3 前缀表达式(Prefix Notation)是指将运算符写在前面、操作数写在后面、不包含括号的表达式，而且为了纪念其发明者波兰数学家 Jan Lukasiewicz 所以前缀表达式也叫做波兰表达式。比如- 1 + 2 3 后缀表达式(Postfix Notation)与之相反，是指运算符写在操作数后面的不含括号的算术表达式，也叫做逆波兰表达式。比如1 2 3 + - 前后缀表达式的出现是为了方便计算机处理，它的运算符是按照一定的顺序出现，所以求值过程中并不需要使用括号来指定运算顺序，也不需要考虑运算符号（比如加减乘除）的优先级。逆波兰表达式在编译技术中有着普遍的应用。 中缀表达式转换成后缀表达式算法： 从左至右扫描一中缀表达式。 若读取的是操作数，则判断该操作数的类型，并将该操作数存入操作数堆栈 若读取的是运算符： 该运算符为左括号”(“，则直接存入运算符堆栈。 该运算符为右括号”)”，则输出运算符堆栈中的运算符到操作数堆栈，直到遇到左括号为止。 该运算符为非括号运算符： 若运算符堆栈栈顶的运算符为括号，则直接存入运算符堆栈。 若比运算符堆栈栈顶的运算符优先级高或相等，则直接存入运算符堆栈。 若比运算符堆栈栈顶的运算符优先级低或者优先级相等，则输出栈顶运算符到操作数堆栈，直到比运算符堆栈栈顶的运算符优先级低或者为空时才将当前运算符压入运算符堆栈。 当表达式读取完成后运算符堆栈中尚有运算符时，则依序取出运算符到操作数堆栈，直到运算符堆栈为空。 流程图如下所示： 逆波兰表达式求值算法： 循环扫描语法单元的项目。 如果扫描的项目是操作数，则将其压入操作数堆栈，并扫描下一个项目。 如果扫描的项目是一个二元运算符，则对栈的顶上两个操作数执行该运算。 如果扫描的项目是一个一元运算符，则对栈的最顶上操作数执行该运算。 将运算结果重新压入堆栈。 重复步骤 2-5，堆栈中即为结果值。 算法实现 中缀表达式转换成逆波兰表达式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758const operatorRand = { '+': 1, '-': 1, '*': 2, '/': 2,};/** * 中缀表达式转换成逆波兰表达式 * @param {string[]} str 中缀表达式 */function convert(inputArr) { if (!Array.isArray(inputArr) || inputArr.length === 0) return []; const operatorArr = []; const outputArr = []; inputArr.forEach(input =&gt; { if (!Number.isNaN(Number(input))) { // 如果是数字，只接输出 outputArr.push(input); } else if (input === '(') { // 如果是左括号，入操作符栈 operatorArr.push(input); } else if (input === ')') { // 如果是右括号，循环输出，知道匹配到左括号为止 while (operatorArr.length &gt; 0) { const operator = operatorArr.pop(); if (operator === '(') break; outputArr.push(operator); } } else { // 如果是运算符 while (operatorArr.length &gt;= 0) { const topOperator = operatorArr[operatorArr.length - 1]; // 如果运算符栈为空，或者栈顶运算符是(，或者当前运算符优先级比栈顶运算符优先级高 if ( operatorArr.length === 0 || topOperator === '(' || operatorRand[input] &gt; operatorRand[topOperator] ) { operatorArr.push(input); break; } else { outputArr.push(operatorArr.pop()); } } } }); // 输入循环结束，如果运算符栈不为空，循环输出 while (operatorArr.length &gt; 0) { outputArr.push(operatorArr.pop()); } return outputArr;} 逆波兰算法求值 123456789101112131415161718192021222324252627282930313233343536373839404142function compute(leftNum, rightNum, operator) { switch (operator) { case '+': return leftNum + rightNum; case '-': return leftNum - rightNum; case '*': return leftNum * rightNum; default: // 除法 return leftNum / rightNum; }}/** * 计算逆波兰表达式 * @param {string} str 逆波兰表达式 */export function count(reversePolishArr) { if (!Array.isArray(reversePolishArr) || reversePolishArr.length === 0) return 0; const tmpArr = []; reversePolishArr.forEach(input =&gt; { if (!Number.isNaN(Number(input))) { // 数字接直接push tmpArr.push(Number(input)); } else { // 运算符 const num1 = tmpArr.pop(); const num2 = tmpArr.pop(); if (isNaN(num1) || isNaN(num2)) { throw new Error(`无效的表达式：${reversePolishArr.join(',')}`); } tmpArr.push(compute(num2, num1, input)); } }); return Number(tmpArr[0].toFixed(3));} 完整代码请参考：https://github.com/zubincheung/js-rpn","link":"/blog/42728bd.html"},{"title":"React中props和state的区别","text":"对于 React 组件而言，数据分为两种： props state React 的数据是自顶向下单向流动的,这两者有什么区别呢？ PropsReact 的核心思想就是组件化思想，组件可以将 UI 切分成一些独立的、可复用的部件，这样你就只需专注于构建每一个单独的部件。 组件从概念上看就是一个函数，它可以接收任意的输入值（称之为”props”），并返回一个需要在页面上展示的 React 元素。所以可以把 props 理解为从外部传入组件内部的数据。由于 React 是单向数据流，所以 props 基本上也就是从服父级组件向子组件传递的数据。 只读性无论是使用函数或是类来声明一个组件，它决不能修改它自己的 props。如果 props 在渲染过程中可以被改变，会导致这个组件显示的形态变得不可预测。只有通过父组件重新渲染的方式才可以把新的 props 传入组件中。所有的 React 组件必须像纯函数那样使用它们的 props。 默认参数defaultProps 可以被定义为在组件类本身上的一个属性，为该类设置默认属性。这对于未定义（undefined）的属性来说有用，而对于设为空（null）的属性并没用。 statestate 是什么呢？ State is similar to props, but it is private and fully controlled by the component. 一个组件的显示形态可以由内部状态和外部参数所决定，props 是组件外传递进来的数据，state 代表的就是 React 组件的内部状态。组件或子组件都不能知道某个组件是有状态还是无状态，并且它们不应该关心某组件是被定义为一个函数还是一个类，组件可以选择将其状态作为属性传递给其子组件。 setStatestate 不同于 props 的一点是，state 是可以被改变的，setState 函数来修改组件 state，而且可以引发组件重新渲染。 不要直接更新状态，不可以直接通过 this.state=XX的方式来修改，应当使用 setState()。 状态更新可能是异步的，你不应该依靠它们的值来计算下一个状态。 状态更新合并，React 可以将多个 setState() 调用合并成一个调用来提高性能。 总结props 是外部传给组件的数据，而 state 是组件内部自己维护的数据，对外部是不可见的。 所以，判断一个数据应该放在哪里，用下面的原则： 如果数据由外部传入，放在 props 中。 如果是组件内部状态，是否这个状态更改应该立刻引发一次组件重新渲染？如果是，放在 state 中。不是，放在成员变量中。 没有 state 的叫做无状态组件，有 state 的叫做有状态组件。 多用 props，少用 state。也就是多写无状态组件。","link":"/blog/3cba66fa.html"},{"title":"记一次MySQL大表空间收缩","text":"最近收到 MySQL 数据库磁盘空间使用率过高的报警，发现其中的一个表空间占用竟然高达三百多 G。删除了一半多的数据后，可是表文件大小还是没变，当时就懵了。请教 DBA 才知道删除大量数据后存在数据空洞，MySQL 不会自动收缩表空间，需要手工操作。 产生原因 InnoDB 里的数据都是用 B+树的结构组织的，每当删除了一条记录后，InnoDB 引擎只会把这个记录标记为删除，而在一段时间内的大量删除操作，会使这种留空的空间变得比存储列表内容所使用的空间更大。 记录的复用只限定符合范围条件的数据。比如说删除一条 ID 为 400 的数据，当执行插入操作 ID 为 400 前后的数据时，MySQL 可能会复用这个位置。但如果某个空白空间一直没有被大小合适的数据占用，仍然无法将其彻底占用，就形成了碎片。 如果是随机插入数据也会造成数据空洞。 解决方案 对数据导出后进行收缩，然后导入数据。 重建表 目前没有导出数据的条件，采用重建表的方法，可以采用alter table A engine=InnoDB 命令。MySQL 5.6 版本开始引入了 Online DDL，允许在表上执行 DDL 的操作（比如创建索引）的同时不阻塞并发的 DML 操作 和 查询（select）操作。 Algorithm=Inplace ：为了避免表拷贝导致的实例性能问题（空间、I/O 问题），建议在 DDL 中包含该选项。如果 DDL 操作不支持 Algorithm=Inplace 方式，DDL 操作会立刻返回错误。 Lock=None ：为了在 DDL 操作过程中不影响业务的 DML 操作，建议在 DDL 中包含该选项。如果 DDL 操作不支持 Lock=None （允许并行 DML 操作）选项，DDL 操作会立刻返回错误。 所有的 DDL 操作均建议在业务低峰期进行，避免对业务产生影响。经过测试，删除表的数据后，对应的磁盘空间也正常释放了。","link":"/blog/6fedc606.html"},{"title":"Jenkins配置NodeJS环境","text":"Jenkins 容器中只有 Java 环境支持运行 jenkins，可通过安装NodeJS 插件提供 Node 运行环境。 下载安装在 系统管理–&gt;管理插件 下载 NodeJS 插件 使用 进入 系统管理–&gt;全局工具配置–&gt;NodeJS 安装 ，可安装多个 NodeJS 版本 构建环境勾选 Provide Node &amp; npm bin/folder to PATH 参考文档NodeJS Plugin","link":"/blog/a0867b89.html"},{"title":"Docker中NPM报错could not get uid&#x2F;gid错误","text":"问题描述最近在 Docker 使用 NPM 安装 pm2 中出现could not get uid/gid错误，错误详情如下： 123456789101112131415161718192021222324252627Error: could not get uid/gid[ 'nobody', 0 ] at /usr/local/lib/node_modules/npm/node_modules/uid-number/uid-number.js:37:16 at ChildProcess.exithandler (child_process.js:296:5) at ChildProcess.emit (events.js:182:13) at maybeClose (internal/child_process.js:962:16) at Process.ChildProcess._handle.onexit (internal/child_process.js:251:5)\u001b[0m\u001b[91mTypeError: Cannot read property 'get' of undefined at errorHandler (/usr/local/lib/node_modules/npm/lib/utils/error-handler.js:205:18) at /usr/local/lib/node_modules/npm/bin/npm-cli.js:78:20 at cb (/usr/local/lib/node_modules/npm/lib/npm.js:228:22) at /usr/local/lib/node_modules/npm/lib/npm.js:266:24 at /usr/local/lib/node_modules/npm/lib/config/core.js:83:7 at Array.forEach (&lt;anonymous&gt;) at /usr/local/lib/node_modules/npm/lib/config/core.js:82:13 at f (/usr/local/lib/node_modules/npm/node_modules/once/once.js:25:25) at afterExtras (/usr/local/lib/node_modules/npm/lib/config/core.js:173:20) at Conf.&lt;anonymous&gt; (/usr/local/lib/node_modules/npm/lib/config/core.js:231:22)\u001b[0m\u001b[91m/usr/local/lib/node_modules/npm/lib/utils/error-handler.js:205 if (npm.config.get('json')) { ^TypeError: Cannot read property 'get' of undefined at process.errorHandler (/usr/local/lib/node_modules/npm/lib/utils/error-handler.js:205:18) at process.emit (events.js:182:13) at process._fatalException (internal/bootstrap/node.js:491:27) 解决方案 在npm install 前加上npm config set unsafe-perm true。 使用用 cli 参数 npm i XX --unsafe-perm。 unsafe-perm 配置查阅npm相关文档：unsafe-perm Default: false if running as root, true otherwise Type: Boolean Set to true to suppress the UID/GID switching when running package scripts. If set explicitly to false, then installing as a non-root user will fail. 针对 unix 平台，使用 root 用户执行 npm 命令时得到的默认值都会是 false。 我的理解大致是避免以 root 的身份去执行时可能造成的安全问题。 参考文档#20861npmWhat does unsafe-perm in npm actually do?","link":"/blog/907c48d9.html"},{"title":"github缩写大全","text":"混Github的时候经常看到类似 LGTM 之类的谜之缩写,这里整理一下这些缩写的含义。 PR: Pull Request. (给项目提交代码) LGTM: Looks Good To Me. (代码看起来不错，可以合并) SGTM: Sounds Good To Me. (同上) WIP: Work In Progress. (提示管理员，代码正在开发中，可以不忙 Code Review，主要应用于非常大的 PR, 分批次提交代码) PTAL: Please Take A Look. (提示别人来看看) TBR: To Be Reviewed. (提示管理员可以进行 Code Review) TL;DR: Too Long; Didn’t Read. (代码太长, 不方便 Code Review) TBD: To Be Done(or Defined/Discussed/Decided/Determined). (用于表示代码开发状态, 例如：完成，讨论中，延期等)","link":"/blog/74188107.html"},{"title":"记一次自动格式化的问题","text":"近日接盘了一个几年前的旧项目，由于 VS Code 开启了 Format On Save 选项，保存的时候自动格式化整个文件的代码。正常情况下格式化代码基本没有任何风险，除非工具有问题，良好的代码格式有利于后面的工作。在本地测试一切正常，推送到测试服务器运行错误，错误详情： 1234567891011SyntaxError: Unexpected token ) at createScript (vm.js:56:10) at Object.runInThisContext (vm.js:97:10) at Module._compile (module.js:542:28) at Object.Module._extensions..js (module.js:579:10) at Module.load (module.js:487:32) at tryModuleLoad (module.js:446:12) at Function.Module._load (module.js:438:3) at Module.runMain (module.js:604:10) at run (bootstrap_node.js:390:7) at startup (bootstrap_node.js:150:9) 错误原因检查发现格式化的时候，一些比较长模板字符串(Template String)自动换行，并在函数后面加了个逗号。函数参数列表的尾后逗号是在 ES2017 添加的，从 Node.Js8.0.0 开始支持这一特性。项目的 Dockerfile 指定了 Node6，本地是 Node10。 解决方法 升级项目中 Node 到 LTS 版本。 使用 eslint + prettier规范代码风格。 去除未使用的代码，少量多次的优化。 写在最后项目尽量使用 lint 工具统一代码风格，改动成本小，--fix 可以尽可能的帮助我们修复老的代码，提高可读性，从而更容易维护。没有银弹，完善的单元测试覆盖更重要。","link":"/blog/7b57214c.html"},{"title":"MongoDB索引笔记","text":"在 MongoDB 查询过程中，索引(Index) 起到非常重要的作用，如果没有索引，MongoDB将会执行全表扫描 。当然如果数据量比较少全表扫描的开销并不大，但如果集合文档数量到百万、千万甚至上亿的时候，一个查询耗费数十秒甚至几分钟都有可能,代价非常高昂。 MongoDB 使用 B-tree 作为索引底层的数据结构。MongoDB 的索引是定义在 集合 （Collection） 级别的,支持对任何单个字段以及嵌套字段和数组建立索引。 MongoDB索引类型默认的 _id indexMongoDB 在 集合（Collection）创建时会默认建立一个基于_id的唯一性索引作为 文档(Document) 的主键，这个索引无法被删除。 单字段索引 （Single Field Index）单字段索引是最常见的索引形式，MongoDB默认创建的id索引也是这种类型，它是有顺序的。 以下操作在records集合的score字段上创建一个升序索引： 1db.records.createIndex( { score: 1 } ) 其中 1 代表升序索引，也可以通过**-1** 来指定降序索引，但是对于单字段索引来说，索引的顺序无关紧要，因为 MongoDB 支持任意顺序查找。 此外 MongoDB 还支持对嵌套字段(Embedded Field) 创建索引： 1db.records.createIndex( { &quot;location.state&quot;: 1 } ) 复合索引 (Compound Index)MongoDB支持对多个字段联合创建索引，称之为复合索引 (Compound Index)。复合索引中字段的顺序很重要,先按第一个字段排序，第一个字段相同的文档按第二个字段排序，依次类推。 以下操作在item字段和stock字段上创建一个升序索引： 1db.products.createIndex( { &quot;item&quot;: 1, &quot;stock&quot;: 1 } ) 索引前缀（Prefixes）prefix 是指索引字段的左前缀子集，如以下索引： 1{ &quot;item&quot;: 1, &quot;location&quot;: 1, &quot;stock&quot;: 1 } 这个索引包含以下索引前缀： { item: 1 } { item: 1, location: 1 } 所以只要语句满足索引前缀都是可以支持使用组合索引的： the item field, the item field and the location field, the item field and the location field and the stock field. 相反如果不满足索引前缀则无法使用索引： the location field the stock field the location and stock fields 如果您的集合同时具有复合索引和其前缀的索引（例如{ a: 1, b: 1 }和{ a: 1 } ），如果两者都没有稀疏或唯一约束，则可以删除前缀上的索引（例如{ a: 1 } ）。 索引的顺序（Sort Order）对于单字段索引，排序的顺序无关紧要，复合索引则完全不一样，排序的顺序必须要和索引一致，逆序之后一致也可以。 对于下面event集合，建立{ username: 1, date: -1 }复合索引： 1db.events.createIndex( { &quot;username&quot;: 1, &quot;date&quot;: -1 } ) 下面列出了上面复合索引支持的查询： db.events.find().sort( { username: 1} ) db.events.find().sort( { username: -1} ) db.events.find().sort( { username: 1, date: -1 } ) db.events.find().sort( { username: -1, date: 1 } ) 其他类型的索引 多键索引 （Multikey Index）当索引的字段为数组时，会自动为这个字段创建一个多键索引，能够加速对数组中元素的查找。 哈希索引（Hashed Index）是指按照某个字段的hash值来建立索引，目前主要用于MongoDB Sharded Cluster的Hash分片，hash索引只能满足字段完全匹配的查询，不能满足范围查询等。 地理位置索引（Geospatial Index）能很好的解决O2O的应用场景，比如『查找附近的美食』、『查找某个区域内的车站』等。 文本索引（Text Index）能解决快速文本查找的需求，比如有一个博客文章集合，需要根据博客的内容来快速查找，则可以针对博客内容建立文本索引。 索引属性（Index Properties）MongoDB索引除了支持的众多索引类型外，还具有各种属性。 TTL Indexes 可以对某个时间字段，指定文档的过期时间。 Unique Indexes 索引字段不出现重复值(如：_id)。 Partial Indexes 只针对符合某个特定条件的文档建立索引。 Case Insensitive Indexes 忽略索引键值的大小写. Sparse Indexes 只针对存在索引字段的文档建立索引。 参考文档MongoDB 索引","link":"/blog/36ce36ae.html"},{"title":"github-invalid-username-or-password","text":"问题描述git clone 一个项目的时候执行命令 1git clone XXX 输入帐号名和密码（没有输错），报以下错误： 1234Username for 'https://github.com': xxxxPassword for 'https://xxxx@github.com':remote: Invalid username or password.fatal: Authentication failed for 'https://github.com/xxxx/xxx.git/' 原因和解决方式如果在 GitHub 中开启了 2FA（two-factor authentication），那么在本地系统中输入 GitHub 账号密码时，不能输入原始的密码（即 GitHub 网站的登录密码），而是需要事先在 GitHub 网站中创建一个 Personal access token，后续在访问代码仓库需要进行权限校验的时候，采用 access token 作为密码进行输入。 參考文档 Creating a personal access token for the command line GitHub: invalid username or password 深入浅出 Git 权限校验","link":"/blog/3575cee4.html"},{"title":"升级babel7踩坑记","text":"项目中升级Babel 7后，运行错误： 1234567891011121314151617181920212223242526272829303132333435363738Unknown substitution &quot;BODY&quot; given at Object.keys.forEach.key (node_modules/@babel/template/lib/populate.js:35:15) at Array.forEach (&lt;anonymous&gt;) at populatePlaceholders (node_modules/@babel/template/lib/populate.js:33:31) at arg (node_modules/@babel/template/lib/string.js:22:51) at arg (node_modules/@babel/template/lib/builder.js:77:14) at spec (node_modules/babel-plugin-transform-es2015-for-of/lib/index.js:171:20) at PluginPass.ForOfStatement (node_modules/babel-plugin-transform-es2015-for-of/lib/index.js:76:21) at newFn (node_modules/@babel/traverse/lib/visitors.js:193:21) at NodePath._call (node_modules/@babel/traverse/lib/path/context.js:53:20) at NodePath.call (node_modules/@babel/traverse/lib/path/context.js:40:17) ============= at exports.default (node_modules/babel-plugin-transform-es2015-for-of/lib/index.js:14:20) at loadDescriptor (node_modules/@babel/core/lib/config/full.js:165:14) at cachedFunction (node_modules/@babel/core/lib/config/caching.js:33:19) at loadPluginDescriptor (node_modules/@babel/core/lib/config/full.js:200:28) at config.plugins.reduce (node_modules/@babel/core/lib/config/full.js:69:20) at Array.reduce (&lt;anonymous&gt;) at recurseDescriptors (node_modules/@babel/core/lib/config/full.js:67:38) at recurseDescriptors (node_modules/@babel/core/lib/config/full.js:94:27) TypeError: Cannot read property 'bindings' of null at Scope.moveBindingTo (node_modules/@babel/traverse/lib/scope/index.js:864:13) at BlockScoping.updateScopeInfo (node_modules/babel-plugin-transform-es2015-block-scoping/lib/index.js:364:17) 1 { at BlockScoping.run (node_modules/babel-plugin-transform-es2015-block-scoping/lib/index.js:330:12) at PluginPass.BlockStatementSwitchStatementProgram (node_modules/babel-plugin-transform-es2015-block-scoping/lib/index.js:70:24) at newFn (node_modules/@babel/traverse/lib/visitors.js:193:21) at NodePath._call (node_modules/@babel/traverse/lib/path/context.js:53:20) at NodePath.call (node_modules/@babel/traverse/lib/path/context.js:40:17) at NodePath.visit (node_modules/@babel/traverse/lib/path/context.js:88:12) at TraversalContext.visitQueue (node_modules/@babel/traverse/lib/context.js:118:16) at TraversalContext.visitSingle (node_modules/@babel/traverse/lib/context.js:90:19) .babelrc 配置： 1{ &quot;presets&quot;: [&quot;env&quot;] } 解决方案：.babelrc改为 1{ &quot;presets&quot;: [&quot;@babel/preset-env&quot;] } 参考链接Upgrade to Babel 7: Cannot read property ‘bindings’ of null","link":"/blog/aa3910a1.html"},{"title":"redis的过期策略和内存淘汰","text":"之前经常将将 Redis 的过期策略和内存淘汰策略搞混淆，查阅了一些资料后做个总结。 过期策略Redis 所有的 key 都可以设置过期时间，时间一到就回自动删除。 如何设置过期时间常用的方式有： EXPIRE key seconds EXPIREAT key timestamp 字符串独有的方式： SET key value [expiration EX seconds|PX milliseconds][nx|xx] SETEX key seconds value Redis 采用的过期策略 惰性删除 在进行 get 或 setnx 等操作时，先检查 key 是否过期， 若过期，删除 key，然后执行相应操作； 若没过期，直接执行相应操作 定期删除依次遍历每个数据库，默认每秒进行十次过期扫描 检查当前库中的随机 20 个 key 删除这 20 个 key 中已经过期的 key； 如果过期的 key 比率超过 1/4，那就重复步骤 1 为了保证过期扫描不会出现循环过度，导致线程卡死现象,会判断定期删除操作是否已经达到指定时长（默认不会超过 25ms），若已经达到，直接退出定期删除。 如果某一个时间段，缓存集中过期失效，对于数据库而言，就会产生周期性的压力波峰造成缓存雪崩。如果有大批量的 key 过期，要给过期时间设置一个随机范围，而不宜全部在同一时间过期，分散过期处理的压力。 内存淘汰机制用于缓存的内存不足时，Redis 提供了几种可选策略 (maxmemory-policy) 来让用户自己决定该如何腾出新的空间以继续提供读写服务。 noeviction 不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。 volatile-lru 尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。 volatile-ttl 跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl 越小越优先被淘汰。 volatile-random 跟上面一样，不过淘汰的 key 是过期 key 集合中随机的 key。 allkeys-lru 区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰。 allkeys-random 跟上面一样，不过淘汰的策略是随机的 key。 如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时不必携带过期时间。如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。 小结Redis 采用惰性删除和定期删除，处理过期的缓存数据。Redis 的内存淘汰策略的选取并不会影响过期的 key 的处理，用于处理内存不足时的需要申请额外空间的数据。 参考文档ExpiresRedis as an LRU cache","link":"/blog/cd3a8f32.html"},{"title":"MySQL临时表","text":"MySQL 临时表分为“内存临时表”和“磁盘临时表”，其中内存临时表使用 MySQL 的 MEMORY 存储引擎，磁盘临时表使用 MySQL 的 MyISAM 存储引擎；一般情况下，MySQL 会先创建内存临时表，但内存临时表超过配置指定的值后，MySQL 会将内存临时表导出到磁盘临时表。MySQL 临时表在我们需要保存一些临时数据时是非常有用的。临时表只在当前连接可见，当关闭连接时，Mysql 会自动删除表并释放所有空间。 创建临时表 给 CREATE TABLE 语句加上 TEMPORARY 关键字即可创建临时表 1234CREATE TEMPORARY TABLE tmp_table ( name VARCHAR(10) NOT NULL, value INTEGER NOT NULL); 直接将查询结果导入并创建临时表 1CREATE TEMPORARY TABLE tmp_table SELECT * FROM table_name 删除临时表默认情况下，当你断开与数据库的连接后，临时表就会自动被销毁。当然你也可以在当前 MySQL 会话使用 DROP TABLE 命令来手动删除临时表。 1DROP TABLE tmp_table; 应用场景 应用场景 1：你在短期内有很多 DML 操作，比如京东淘宝亚马逊的购物车表，把东西放购物车（insert），变更数量（update），删除商品(delete)，一旦结算金钱后，这些数据就要清掉，这时需要用临时表。 应用场景 2：在导出数据时，你可能不想导完整的数据库，或者表，你可能只想要导出符合某些条件的数据，那么你可以创建临时表，把 select 语句插入到临时表，接着导出这个临时表，导完以后通过结束 session 或者事务的方式，让这些没用的数据自动清理掉。 应用场景 3：你在写存储过程时，有很多的连接，比如你需要连接 A,B,C,D,E,F,G,H 那么多张表，才能得到你的结果表，同时做连接的消耗太大，你可以先 A,B,C 连接的结果，放在临时表，接着再把这张临时表，跟 D,E,F 连接，作为新的结果放在临时表，接着再把临时表与 G,H 连接，最后得到临时表数据，一次插入到结果表（永久表）。原文 延伸阅读什么情况下会用到临时表官方手册","link":"/blog/89fba6c2.html"},{"title":"Express查询字符串数组解析为对象的问题","text":"这几天检查日志，每天总会有大量上游的调用请求出现400错误（Expected type array but found type object）, 这个API Get请求参数如下: 1?arr[]=value1&amp;arr[]=value2&amp;arr[]=value3... 进一步分析日志发现，上游的查询参数中数组是有指定索引的，比如 1?arr[0]=value1&amp;arr[1]=value2&amp;arr[2]=value3... 而且奇怪的是并非所有的请求都有问题，只有当arr[]超过20个才会出现，第一反应是Express的query parser问题,将查询字符串数组解析为对象， 原来Express已经不包含大部分的请求解析中间件了，如json、urlencoded、cookie等中间件都变成可配置的了，只有查询字符串解析中间件还是内置的,其中解析函数默认为qs模块的，可以通过app上的query parse设置。 qs可以根据[]来解析数组，也支持指定索引。 12qs.parse('a[]=b&amp;a[]=c'); // { a: ['b', 'c'] }qs.parse('a[1]=c&amp;a[0]=b'); // { a: ['b', 'c'] } 查询qs的文档发现： qs will also limit specifying indices in an array to a maximum index of 20. Any array members with an index of greater than 20 will instead be converted to an object with the index as the key. This is needed to handle cases when someone sent, for example, a[999999999] and it will take significant time to iterate over this huge array. qs默认数组长度只到20， 所以当指定的索引超过20时，整个属性会被当作对象处理 1qs.parse('a[100]=b'); // { a: { '100': 'b' } } 可以通过修改arrayLimit 来指定允许解析的数组的最大索引。 12qs.parse('a[1]=b&amp;a[2]=1', { arrayLimit: 0 }) // { a: { 1: 'b', 2: 1 } }qs.parse('a[1]=b&amp;a[2]=1', { arrayLimit: 2 }) // { a: ['b', 1] } 解决方法：修改Express的的query parse设置，代码如下 1234567const express = require('express');const qs = require('qs');const app = express();app.set('query parser', function (str) { return qs.parse(str, opts);}); 参考连接 Express.js API reference Make it possible to configure the querystring parser options Parsing Arrays","link":"/blog/33429338.html"},{"title":"API多版本控制","text":"现状移动互联网时代，面对市场变化，产品必须不停迭代，而在升级中我们业务需求可能不断在更新，但是我们势必又要保证原来功能的可用性，不能因为用户不升级而导致旧版本无法使用，只能引导用户去更新，而不是强制用户升级。所以我们需要对产品多版本做兼容，解决的核心方向在于服务端API的多版本管理。 存在的问题 每个API如何进行灵活的版本管理，多个版本的共存问题 API多个版本共存维护成本越来越高 API更新时需要兼容之前的版本，导致参数会越来越多 API更新后会有很多关于版本的逻辑判断 常见的API版本管理方式API版本控制主要有下面几种方式。 The Knot: 不设定版本每个API只有一个版本，所有的用户都必须使用最新的API。如果直接在原有API上修改，这就意味着所有的用户都必须使用新的版本，需要强制用户更新最新版本的APP，非常影响用户的体验。并且这也会与业务冲突，很多场景业务就有需要新老系统并行的要求。 可选的执行方案： 新的API名称新功能使用新的API名字，新版本调用新名称API，例如： 12https://api.example.com/foo/barhttps://api.example.com/foo/newbar 但是随着版本迭代的次数越来越多，API接口数量也会越来越多，项目冗余代码日益增多，代码变得难以维护。 Point-to-Point: API自带版本每个API可以有多个不同的版本，API调用方可以根据自己的需要调用不同版本的API。多个版本的API可以共存，老版本的用户不会受到新版本更新的影响。这种方式只有在系统比较稳定的时候，产品初期版本更新迭代比较快则不太适合，而且如果是每个版本都单独部署的话维护比较麻烦也比较浪费资源。 可选执行方案： URI中路径添加版本号 12https://api.example.com/v1/foo/barhttps://api.example.com/v2/foo/bar URI参数添加版本号 1https://www.example.com/api/foo/bar?version=v1 使用子域名 12https://v1.api.example.com/foo/barhttps://v2.api.example.com/foo/bar Compatible Versioning: 兼容性版本控制每个API只提供一个版本，所有用户都调用同一个API，如果要修改API需要兼容旧版本的所有功能。 可选的执行方案： 在请求头中添加版本号 123GET /foo/bar HTTP/1.1Host: api.example.comAccept-Version: v1 123GET /foo/bar HTTP/1.1Host: api.example.comAccept: application/vnd.example+json;version=1.0 123GET /foo/bar HTTP/1.1Host: api.example.comAccept: application/vnd.example.v1+json 客户端token添加版本号 1https://api.example.com/foo/bar?token=fc0bfd66c2f3691274982e5f085c2c9c5e54d5b43c5af969395722a74912133c 在服务端处理token的时候，确定请求API的内部版本，再执行具体API。 总结每个API版本控制的方案没有绝对的好坏，关键在于是否适合所在的场景。有很多业务需要做版本控制，业务代码兼容过多版本，又会造成代码难以维护的局面，通过运维做多版本控制来处理API服务多版本控制，是一种非常值得推荐的方案。如果维护的app或者说是API版本过多，必然会导致维护成本大大提高。我们应该统计每个版本的使用量，把使用量较少的版本给干点。另一方面，一些过时的版本我们并不希望用户继续使用时，也可以考虑强制升级。","link":"/blog/3fda6034.html"},{"title":"redis cluster的批量操作","text":"之前项目中都在使用redis的单机或者主从，后来迁移到集群的时候，出现了一系列问题，下面整理一下redis集群方面的知识。 Redis集群介绍redis cluster是去中心化的，每个节点都是master,将数据按key哈希到16384个slot上，每个redis node负责一部分的slot。 按照CAP理论来说，**单机版的Redis属于保证CP(Consistency &amp; Partition-Tolerancy)而牺牲A(Availability)**，也就说Redis能够保证所有用户看到相同的数据（一致性，因为Redis不自动冗余数据）和网络通信出问题时，暂时隔离开的子系统能继续运行（分区容忍性，因为Master之间没有直接关系，不需要通信），但是不保证某些结点故障时，所有请求都能被响应（可用性，某个Master结点挂了的话，那么它上面分片的数据就无法访问了）。 有了Cluster功能后，Redis从一个单纯的NoSQL内存数据库变成了分布式NoSQL数据库，CAP模型也从CP变成了AP。也就是说，通过自动分片和冗余数据，Redis具有了真正的分布式能力，某个结点挂了的话，因为数据在其他结点上有备份，所以其他结点顶上来就可以继续提供服务，保证了可用性(Availability)。然而，也正因为这一点，Redis无法保证曾经的强一致性了。这也是CAP理论要求的，三者只能取其二。 Redis集群的功能限制Redis集群相对单机在功能上存在一些限制，需要开发人员提前了解，在使用时做好规避。 key批量操作支持有限。类似 mset、mget 操作，目前只支持对具有相同 slot 值的 key 执行 批量操作。对于映射为不同 slot 值的 key 由于执行 mget、mget 等操作可能存在于多个节点上，因此不被支持。 key事务操作支持有限。只支持多key在同一节点上的事务操作，当多个key分布在不同的节点上时无法使用事务功能。 key 作为数据分区的最小粒度不能将一个大的键值对象如 hash、list 等映射到不同的节点。 不支持多数据库空间单机下的Redis可以支持16个数据库（db0~db15），集群模式下只能使用一个数据库空间，即db0。 复制结构只支持一层从节点只能复制主节点，不支持嵌套树状复制结构。 参考链接 cluster-tutorial Redis集群模式搭建与原理详解","link":"/blog/2669ab64.html"},{"title":"TSL握手小记","text":"超文本传输安全协议（英语：Hypertext Transfer Protocol Secure，缩写：HTTPS，常称为 HTTP over TLS，HTTP over SSL 或 HTTP Secure）是一种通过计算机网络进行安全通信的传输协议。 HTTPS 经由 HTTP 进行通信，但利用 SSL/TLS 协议来加密数据包。 TLS 协议位于传输层之上，应用层之下，使用了两种加密技术，分别为：对称加密和非对称加密。使用非对称密钥加密用于传输对称密钥来保证传输过程的安全性，之后使用对称密钥加密进行通信来保证通信过程的效率。 对称加密加密与解密使用相同密钥。 优点：运算速度快； 缺点：无法安全地将密钥传输给通信方。 非对称加密加密与解密使用不同密钥，包含一个公钥和一个私钥。明文既可以用公钥加密，用私钥解密；还可以用来进行签名，用私钥加密，用公钥解密。 优点：运算速度快； 缺点：无法安全地将密钥传输给通信方。 TLS 握手过程 客户端先向服务端发送一个加密通信请求，叫做 ClientHello 请求。该请求包含以下信息： 客户端支持的 SSL 或者 TLS 版本 客户端生成的随机数 客户端支持的加密算法 服务端收到客户端请求后，向客户端发出响应，叫做 ServerHello。该响应包含以下信息： 服务端从客户端提供的 SSL 或 TLS 列表中选择的版本 Sesstion ID 和 另外生成的随机数 服务端的数字证书 确认使用的加密算法 客户端收到服务端响应后，首先校验服务端发来的数字证书决定是否继续通信。 证书校验通过，会像服务端发送以下信息： 生成一个随机数，并对这个随机数用从服务端数字证书中取出的公钥加密 如果服务端发送了一个客户端证书请求，客户端将会发送一个用客户端私钥加密的随机字符串和客户端的数字证书，或者没有数字证书的警告。 服务端接受并验证客户端证书，通过这三个随机值按照之前约定的加密方式生成密钥。 客户端向服务端发送一条完成的消息，该消息使用密钥加密，表示握手的客户端部分已经完成。 服务端向客户端发送一条完成的消息，该消息使用密钥加密，表示握手的服务端部分已经完成 在 SSL 或 TLS 会话期间，服务端和客户端现在可以交换使用共享密钥对称加密的消息","link":"/blog/9a691d3d.html"},{"title":"常用的字符编码","text":"一、基础知识计算机中储存的信息都是用二进制数表示的，位（bit）是指计算机里存放的二进制值(0/1)，而 8 个位组合成的“位串”称为一个 字节字符集（Charset）：是一个系统支持的所有抽象字符的集合。字符是各种文字和符号的总称，包括各国家文字、标点符号、图形符号、数字等。字符编码（Character encoding）也称字集码，是把字符集中的字符编码为指定集合中某一对象（例如：比特模式、自然数序列、8 位组或者电脉冲），以便文本在计算机中存储和通过通信网络的传递。 二、ASCIIASCII 的全称是 American Standard Code for Information Interchange（美国信息交换标准代码）。ASCII 编码只支持基础拉丁字符，主要用于显示现代英语。ASCII 用一个字节（8 个位）来表示一个字符，并保证最高位的取值永远为’0’。即表示字符含义的位数为 7 位，不难算出其可表达字符数为 2^7 =128 个。这 128 个字符包括 95 个可打印的字符（涵盖了 26 个英文字母的大小写以及英文标点符号能）与 33 个控制字符（不可打印字符）。为了解决西欧的字符编码问题，用上第 8 位，这样能表达的字符个数就达到了 2^8 =256 个，相比较原来，增长了一倍， 这个编码规则也常被称为 EASCII 三、中文编码 GB2312（国家简体中文字符集）用两个字节来表示一个汉字（注意是表示一个汉字，对于拉丁字母，GB2312 还是是用一个字节来表示以兼容 ASCII）。GB2312 能表示 7445 个符号，包括 6763 个汉字。 BIG5 (统一繁体字符集)，能表示 21886 个符号，兼容 ASCII，但与 GB2312 有冲突。 GBK (汉字内码扩展规范) GB2312 的扩展，支持繁体字，能表示 21886 个符号。 GB18030 (中文编码字符集)，是中华人民共和国现时最新的内码字集，与 GB2312 完全兼容，与 GBK 基本兼容，支持 GB13000 及 Unicode 的全部统一汉字 四、Unicode针对不同的语言采用不同的编码，有可能导致冲突与不兼容性，为了能独立表示世界上所有的字符，Unicode 采用 4 个字节表示一个字符,这样理论上 Unicode 能表示的字符数就达到了 2^31 = 2147483648 = 21 亿左右个字符，完全可以涵盖世界上一切语言所用的符号。 缺点：Unicode 对所有的字符编码均需要四个字节，而这对于拉丁字母或汉字来说其前面三个或两个字节均是 0,这对信息存储来说是极大的浪费。另外一个问题就是，如何区分 Unicode 与其它编码这也是一个问题， UTF-8（8-bit Unicode Transformation Format）是一种针对 Unicode 的可变长度字符编码（定长码）,UTF-8 使用一至四个字节为每个字符编码 UTF-8 是 Unicode 的一种实现方式，而 Unicode 是一个统一标准规范，Unicode 的实现方式除了 UTF-8 还有其它的，比如 UTF-16 等 参考文档字符编码常识及问题解析字符集和字符编码（Charset &amp; Encoding）","link":"/blog/29e2a71f.html"},{"title":"关于Rest和RPC","text":"在设计API的时候，很多人会拿 REST 与 RPC 相比较，发现自己对这两者并不是很理解，于是查阅了网上相关资料加上自己的理解写下本篇文章以加深印象。 什么是REST表现层状态转换（Representational State Transfer，缩写：REST）源自Roy Thomas Fielding博士于2000年发表的博士论文，目的是便于不同软件/程序在网络（例如互联网）中互相传递信息。表现层状态转换是根基于超文本传输协议（HTTP）之上而确定的一组约束和属性，是一种设计提供万维网络服务的软件构建风格。符合或兼容于这种架构风格（简称为 REST 或 RESTful）的网络服务，允许客户端发出以统一资源标识符访问和操作网络资源的请求，而与预先定义好的无状态操作集一致化。因此表现层状态转换提供了在互联网络的计算系统之间，彼此资源可交互使用的协作性质（interoperability）。相对于其它种类的网络服务，例如SOAP服务，则是以本身所定义的操作集，来访问网络上的资源。 REST架构一套理想的、完全满足REST风格的系统应该满足以下六大原则： 客户端-服务器分离（Client-Server）客户端-服务器结构限制的目的是将客户端和服务器端的关注点分离。将用户界面所关注的逻辑和数据存储所关注的逻辑分离开来有助于提高用户界面的跨平台的可移植性。通过简化服务器模块也有助于服务器模块的可扩展性。 无状态（Stateless）无状态是REST的一条核心原则，服务器不能保存客户端的信息；每一次从客户端发送的请求中，要包含所有的必须的状态信息，会话信息由客户端保存，服务器端根据这些状态信息来处理请求。客户端承担状态维护职责后，可能会产生一些新的问题，比如身份认证、授权等可信问题，需要一些针对性的解决方案。服务器可以将会话状态信息传递给其他服务，比如数据库服务，这样可以保持一段时间的状态信息，从而实现认证功能。 可缓存（Cacheability）使用无状态的设计原则可能会需要多次请求，或者在请求中带有额外的冗余信息。如同万维网一样，客户端和中间的通讯传递者可以将回复缓存起来，管理良好的缓存机制可以减少客户端-服务器之间的交互，甚至完全避免客户端-服务器交互，这进一步提了高性能和可扩展性。 统一接口（Uniform Interface）这是 RESTful 系统设计的另一条核心原则。它简化了系统架构，减少了耦合性，可以让所有模块各自独立的进行改进。包括下列四个限制： 请求中包含资源的 ID（Resource identification in requests） 请求中包含了各种独立资源的标识，例如，在Web服务中的URI。资源本身和发送给客户端的标识是独立。例如，服务器可以将自身的数据库信息以HTML、XML或者JSON的方式发送给客户端，但是这些可能都不是服务器的内部记录方式。 资源通过标识来操作（Resource manipulation through representations）当客户端拥有一个资源的标识，包括附带的元数据，则它就有足够的信息来删除这个资源。 消息的自我描述性（Self-descriptive messages）每一个消息都包含足够的信息来描述如何来处理这个信息. 例如，媒体类型 (media-type) 就可以确定需要什么样的分析器来分析媒体数据. 用超媒体驱动应用状态（Hypermedia as the engine of application state (HATEOAS)）同用户访问Web服务器的Home页面相似，当一个 REST 客户端访问了最初的REST应用的URI之后，REST 客户端应该可以使用服务器端提供的链接，动态的发现所有的可用的资源和可执行的操作。随着访问的进行，服务器在响应中提供文字超链接，以便客户端可以得到当前可用的操作。客户端无需用确定的编码的方式记录下服务器端所提供的动态应用的结构信息。 分层系统（Layered System）客户端一般不知道是否直接连接到了最终的服务器，或者是路径上的中间服务器。中间服务器可以通过负载均衡和共享缓存的机制提高系统的可扩展性，这样可也便于安全策略的部署。 按需代码（Code-On-Demand，可选）服务器可以通过发送可执行代码给客户端的方式临时性的扩展功能或者定制功能，例如Java Applet、Flash或JavaScript。 REST的优点 可更高效利用缓存来提高响应速度 通讯本身的无状态性可以让不同的服务器的处理一系列请求中的不同请求，提高服务器的扩展性 浏览器即可作为客户端，简化软件需求 相对于其他叠加在HTTP协议之上的机制，REST的软件依赖性更小 不需要额外的资源发现机制 在软件技术演进中的长期的兼容性更好 REST的不足 REST 与 HTTP 完全绑定，不适合应用于要求高性能传输的场景中 REST 没有传输可靠性支持 REST 难以应对复杂的业务逻辑，缺乏对自然进行”部分”和”批量”处理的能力 目前，一种理论上较优秀的可以解决以上这几类问题的方案是GraphQL。 什么是RPC分布式计算中，远程过程调用（英语：Remote Procedure Call，RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一个地址空间（通常为一个开放网络的一台计算机）的子程序，而程序员就像调用本地程序一样，无需额外地为这个交互作用编程（无需关注细节）。RPC是一种服务器-客户端（Client/Server）模式，经典实现是一个通过发送请求-接受回应进行信息交互的系统。 进程间的通信不可否认，RPC 出现的最初目的就是为了让计算机能够与调用本地方法一样调用远程方法。程序分布在不同的地址空间里。如果在同一主机里，RPC可以通过不同的虚拟地址空间（即便使用相同的物理地址）进行通讯，而在不同的主机间，则通过不同的物理地址进行交互。 如何表示数据无论是讲参数传递给另外一个进程，还是从另外一个进程中取回执行结果，都涉及数据的序列化和反序列化问题，常见的有： Java RMI 的 Java 对象序列化流协议 gRPC 的 Protocol Buffers Web Service 的 XML 序列化 众多轻量级 RPC 支持的 JSON 序列化 如何传递数据 HTTP协议A服务器的应用可以通过HTTP将数据传输到B服务器，B服务器接收到数据后执行数据中调用的指定方法、函数，例如谷歌的gRPC就是在HTTP上进行数据传输的。但是由于HTTP报头中有太多不需要的信息造成带宽的浪费，所以很多人都是用比HTTP传输效率高的TCP、UDP进行数据传输。 TCP、UDP例如著名的Netty就是基于TCP、UDP上进行传输的，当然你也可以不使用框架，自己编写Socket实现网络数据传输。 如何表示方法 同步调用A服务器的应用调用B服务器上应用的方法、函数后，A服务器的应用会处在阻塞状态，只有等到B服务器上的应用通过网络返回结果后，A服务器的应用才会继续往下执行。 异步调用A服务器的应用调用B服务器上应用的方法、函数后，A服务器的应用并不会进入阻塞状态等待结果的返回，可以通过回调通知等方式获得返回的结果。 对比和区别其实，REST 无论是在思想上、概念上还是在使用范围上，跟 RPC 都不尽相同，只能算是有一点相似。 REST 和 RPC 从思想上差异的核心是抽象的目标，RPC 是面向方法的 ，而 REST 是面向资源的。 REST 和 RPC 在概念上的不同是指 REST 它不是一种协议(协议都带有一定的规范性和强制性)，只是一种软件架构风格，尽管有一些指导规则，并不受任何强制的约束。 至于使用范围，REST 和 RPC 都常用于微服务架构中，作为主流的两种远程调用方式，REST 接口更加规范，通用适配性要求高，建议对外的接口都统一成 REST，而组件内部的各个模块，可以选择 RPC。","link":"/blog/252c89c5.html"},{"title":"ubuntu下minikube安装","text":"尝试在自己的笔记本搭建 Kubernetes 环境，于是用 minikube 搭建一个单节点集群，我的系统是ubuntu20.04，安装步骤可以查看官方文档：Install Minikube。 安装minikube的过程中问题还比较多的，首先就是运行 minikube start 的时候，终端报出以下错误： 1234567891011😄 minikube v1.24.0 on Ubuntu 20.04 (arm64)👎 Unable to pick a default driver. Here is what was considered, in preference order: ▪ docker: Not healthy: &quot;docker version --format {{.Server.Os}}-{{.Server.Version}}&quot; exit status 1: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get &quot;http://%2Fvar%2Frun%2Fdocker.sock/v1.24/version&quot;: dial unix /var/run/docker.sock: connect: permission denied ▪ docker: Suggestion: Add your user to the 'docker' group: 'sudo usermod -aG docker $USER &amp;&amp; newgrp docker' &lt;https://docs.docker.com/engine/install/linux-postinstall/&gt; ▪ kvm2: Not installed: exec: &quot;virsh&quot;: executable file not found in $PATH ▪ vmware: Not installed: exec: &quot;docker-machine-driver-vmware&quot;: executable file not found in $PATH ▪ podman: Not installed: exec: &quot;podman&quot;: executable file not found in $PATH ▪ virtualbox: Not installed: unable to find VBoxManage in $PATH❌ Exiting due to DRV_NOT_HEALTHY: Found driver(s) but none were healthy. See above for suggestions how to fix installed drivers. 我们可以使用 --vm-driver=none 选项，完整命令如下： 1sudo minikube start --vm-driver=none 又碰到一个新的问题就是： 12345😄 minikube v1.24.0 on Ubuntu 20.04 (arm64)✨ Using the none driver based on user configuration❌ Exiting due to GUEST_MISSING_CONNTRACK: Sorry, Kubernetes 1.22.3 requires conntrack to be installed in root's path 提示的很清楚了，需要安装 conntrack： 1sudo apt install conntrack 此外使用官方的方法来安装可能会出现网络问题，所以推荐使用阿里云的镜像： 1sudo minikube start --vm-driver=none --registry-mirror=https://registry.docker-cn.com","link":"/blog/7e9f8faa.html"},{"title":"基于Ubuntu安装部署Kubernetes集群","text":"环境准备 安装 Ubuntu 虚拟机通过 Parallels Desktop 安装了 3 台虚拟机，并对虚拟机进行如下规划： Hostname IP OS Specs k8s-master-01 192.168.11.190 Ubuntu 20.04 2GB Ram, 2vcpus k8s-node-01 192.168.11.191 Ubuntu 20.04 2GB Ram, 2vcpus k8s-node-02 192.168.11.192 Ubuntu 20.04 2GB Ram, 2vcpus 科学上网 关闭swap 编辑 /etc/fstab 文件，注释掉引用 swap 123sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstabsudo swapoff -a 安装和配置 Docker在每台虚拟机上安装 Docker, 参考Install Docker Engine on Ubuntu： 12345678910111213141516171819202122232425262728293031323334353637# 更正 apt 包索引和安装GPG证书sudo apt updatesudo apt install -y \\ ca-certificates \\ curl \\ gnupg \\ lsb-releasecurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgecho \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null# 更新 apt 包索引并安装 Dockersudo apt updatesudo apt install -y docker-ce docker-ce-cli containerd.io# 配置 Docker 服务sudo mkdir -p /etc/systemd/system/docker.service.dsudo tee /etc/docker/daemon.json &lt;&lt;EOF{ &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot;}EOF# 重启 Docker 服务sudo systemctl daemon-reloadsudo systemctl restart dockersudo systemctl enable docker 安装和配置 Kubernetes在每台虚拟机上安装 kubeadm、kubelet 和 kubectl, 参考 Installing kubeadm： 1234567891011121314# 更新 apt 包索引并安装使用 Kubernetes apt 仓库所需要的包：sudo apt updatesudo apt install -y apt-transport-https ca-certificates curl# 下载 Google Cloud 公开签名秘钥：sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg# 添加 Kubernetes apt 仓库：echo &quot;deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.list# 更新 apt 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：sudo apt updatesudo apt install -y kubelet kubeadm kubectlsudo apt-mark hold kubelet kubeadm kubectl 如果 packages.cloud.google.com 无法访问的话，可以更换换国内的源： 12345sudo curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -echo &quot;deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main&quot; | sudo tee /etc/apt/sources.list.d/kubernetes.listsudo apt update 配置节点初始化 Master 节点123456789sudo kubeadm resetsudo systemctl enable kubeletsudo kubeadm init \\ --pod-network-cidr=192.168.0.0/16 \\ --upload-certs \\ --node-name=k8s-master-01 \\ --image-repository registry.aliyuncs.com/google_containers –-pod-network-cidr: 指定pod网络的IP地址范围，它的值取决于你在下一步选择的哪个网络网络插件，这里使用 Calico, 参考 Quickstart for Calico on Kubernetes。 –dry-run 当使用kubeadm命令初始化集群时，会去官方镜像仓库（k8s.gcr.io）拉取镜像。但是国内网络无法访问官方镜像仓库，导致集群初始化失败，可以使用其他镜像仓库。 kubeadm init 输出的token用于master和加入节点间的身份认证，token是机密的，需要保证它的安全，因为拥有此标记的人都可以随意向集群中添加节点。 配置 kubectl： 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 安装 Calico 网络插件 123kubectl create -f https://projectcalico.docs.tigera.io/manifests/tigera-operator.yamlkubectl create -f https://projectcalico.docs.tigera.io/manifests/custom-resources.yaml 修改Docker及kubelet的Cgroup Driver在安装kubernetes的过程中，经常会遇见如下错误 failed to create kubelet: misconfiguration: kubelet cgroup driver: “cgroupfs” is different from docker cgroup driver: “systemd” 原因是 docker 的 Cgroup Driver 和 kubelet 的 Cgroup Driver 不一致。有两种方式解决问题，一种是修改docker,，另一种是修改kubelet； 修改 Docker 的 Cgroup Driver修改/etc/docker/daemon.json文件 123{ &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]} 重启docker 12systemctl daemon-reloadsystemctl restart docker 修改 kubelet 的 Cgroup Driver 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf文件，增加–cgroup-driver=cgroupfs 1Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroup-driver=cgroupfs&quot; 重启kubelet 12systemctl daemon-reloadsystemctl restart kubelet 卸载集群在主节点上运行： 123kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsetskubectl delete node &lt;node name&gt; 然后在需要移除的节点上，重置kubeadm的安装状态： 1kubeadm reset","link":"/blog/93359efa.html"},{"title":"Linux平均负载","text":"一、什么是平均负载平均负载（Load Average）是指一段时间内，系统处于可运行状态和不可中断状态的平均进程数，这个一段时间一般取 1 分钟、5 分钟、15 分钟。 CPU 使用率是单位时间内 CPU 繁忙情况的统计，跟平均负载并不完全对应，平均负载不仅包括正在使用 CPU 的进程，还包括等待 CPU 和等待 I/O的进程，比如： CPU 密集型进程，使用大量 CPU 回导致平均负载升高 I/O 密集型进程，等待 I/O 也会使平均负载升高，但 CPU 使用率不一定高 大量等待 CPU 的进程调度也会导致平均负载升高，此时 CPU 使用率也会比较高 如何查看平均负载top，uptime，w 等命令都可以查看系统负载： 12$ uptime12:04 up 8 days, 2:33, 2 users, load averages: 4.14 5.85 5.24 二、平均负载实际应用获取 CPU 核心数考虑了 CPU 核心数的影响，才能解释系统负载。可以使用 nproc 或 lscpu 命令查看系统中的处理器单元数量。 1234$ nproc4# 或者lscpu 或者使用 grep 命令查看 /proc/cpuinfo 12$ grep 'model name' /proc/cpuinfo | wc -l4 平均负载多少比较合理那么在实际生产环境中，平均负载多少比较合理呢，个人认为： 平均负载低于 CPU 数量的 70%的时候，系统还是比较空闲的 平均负载高于 CPU 数量 70%的时候，你就应该分析负载高的原因了 平均负载高于 CPU 数量的时候意味着有部分进程竞争不到 CPU,将会有性能问题。 但是 70%并不是绝对的，我们需要分析系统负载趋势，才能更全面的处理问题，结合具体情况具体分析，假设单核情况下： 1 分钟 Load&gt;5，5 分钟 Load&lt;1，15 分钟 Load&lt;1：短期内繁忙，中长期空闲，初步判断是一个“抖动”或者是“拥塞前兆” 1 分钟 Load&gt;5，5 分钟 Load&gt;1，15 分钟 Load&lt;1：短期内繁忙，中期内紧张，很可能是一个“拥塞的开始” 1 分钟 Load&gt;5，5 分钟 Load&gt;5，15 分钟 Load&gt;5：短中长期都繁忙，系统“正在拥塞” 1 分钟 Load&lt;1，5 分钟 Load&gt;1，15 分钟 Load&gt;5：短期内空闲，中长期繁忙，不用紧张，系统“拥塞正在好转” 三、如何分析排查平均负载 运行 uptime 或者 top 命令查看平均负载的变化情况 12$ watch -d uptime14:55:16 up 5:39, 7 users, load average: 0.15, 0.05, 0.01 运行 mpstat 查看 CPU 使用率的变化 12345678910# -P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据$ mpstat -P ALL 5Linux 4.8.0-54-generic (zubin-pc) 2021年07月18日 _x86_64_ (4 CPU)14时56分15秒 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle14时56分20秒 all 0.20 0.00 0.15 0.00 0.00 0.00 0.00 0.00 0.00 99.6514时56分20秒 0 0.00 0.00 0.00 0.00 0.00 0.20 0.00 0.00 0.00 99.8014时56分20秒 1 0.20 0.00 0.40 0.00 0.00 0.00 0.00 0.00 0.00 99.4014时56分20秒 2 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.8014时56分20秒 3 0.60 0.00 0.40 0.00 0.00 0.00 0.00 0.00 0.00 99.00 运行 pidstat 查看进程的 CPU、内存、线程、设备 IO 等系统资源的占用情况 123456789101112$ pidstat -u 5Linux 4.8.0-54-generic (zubin-pc) 2021年07月18日 _x86_64_ (4 CPU)14时57分20秒 UID PID %usr %system %guest %CPU CPU Command14时57分25秒 0 1118 0.60 0.40 0.00 1.00 2 Xorg14时57分25秒 0 1499 0.00 0.20 0.00 0.20 1 vmtoolsd14时57分25秒 1000 1830 0.40 0.20 0.00 0.60 3 compiz14时57分25秒 1000 2310 0.00 0.40 0.00 0.40 1 /usr/bin/x-term14时57分25秒 1000 7441 0.20 0.00 0.00 0.20 1 pidstat14时57分25秒 1000 8169 0.00 0.20 0.00 0.20 0 iostat14时57分25秒 1000 8901 0.20 0.20 0.00 0.40 3 top14时57分25秒 1000 10034 0.20 0.20 0.00 0.40 3 pidstat","link":"/blog/6560bd3f.html"},{"title":"MySQL索引小结","text":"索引的作用索引的出现其实就是为了提高数据查询的效率，用于快速找出在某个列中有一特定值的行。 索引的常见模型实现索引的方式却有很多种,常见的索引模型有哈希表、有序数组、搜索树 哈希表：键 - 值(key - value) 哈希的思路：把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置 哈希冲突的处理办法：链表 适用场景：只有等值查询的场景 有序数组：按顺序存储。 查询用二分法就可以快速查询，时间复杂度是：O(log(N)) 优点：在等值查询和范围查询场景中的性能就都非常优秀 缺点：往中间插入一个记录就必须得挪动后面所有的记录，更新效率低 适用场景：静态存储引擎。 二叉搜索树 特点：每个节点的左儿子小于父节点，父节点又小于右儿子 查询时间复杂度： O(log(N)) 更新时间复杂度： O(log(N)) 适用场景：数据库存储大多不适用二叉树，因为树高过高，会适用 N 叉树 InnoDB 中的索引模型在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。 索引类型：主键索引、非主键索引 主键索引的叶子节点存的是整行的数据(聚簇索引) 非主键索引的叶子节点内容是主键的值(二级索引) 区别：主键索引只要搜索 ID 这个 B+Tree 即可拿到数据。普通索引先搜索索引拿到主键值，再到主键索引树搜索一次(回表) 索引维护 一个数据页满了，按照 B+Tree 算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概 50%。 当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。 从性能和存储空间方面考量，自增主键往往是更合理的选择。 覆盖索引回到主键索引树搜索的过程，我们称为回表 如果查询条件使用的是普通索引（或是联合索引的最左原则字段），查询结果是联合索引的字段或是主键，不用回表操作，直接返回结果，减少 IO 磁盘读写读取正行数据 最左前缀联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符 联合索引根据创建联合索引的顺序，以最左原则进行 where 检索，比如（age，name）以 age=1 或 age= 1 and name=‘张三’可以使用索引，单以 name=‘张三’ 不会使用索引，考虑到存储空间的问题，还请根据业务需求，将查找频繁的数据进行靠左创建索引。 索引下推like 'hello%’and age &gt;10 检索，MySQL5.6 版本之前，会对匹配的数据进行回表查询。5.6 版本后，会先过滤掉 age&lt;10 的数据，再进行回表查询，减少回表率，提升检索速度","link":"/blog/43a71ae4.html"},{"title":"MySQL索引优化","text":"不使用索引的情况 如果 mysql 估计使用索引比全表扫描更慢，则不使用索引。例如：如果 key_part 1 均匀分布在 1 和 100 之间，下列查询中使用索引就不是很好： 1SELECT * FROM table_name where key_part1 &gt; 1 and key_part1 &lt; 90; 如果使用 heap 表并且 where 条件中不用＝索引列，其他 &gt; 、 &lt; 、 &gt;= 、 &lt;= 均不使 用索引（MyISAM 和 innodb 表使用索引）； 使用 or 分割的条件，如果 or 前的条件中的列有索引，后面的列中没有索引，那么涉及到的索引都不会使用。 如果创建复合索引，如果条件中使用的列不是索引列的第一部分；（不是前缀索引） 如果 like 是以％开始； 对 where 后边条件为字符串的一定要加引号，字符串如果为数字 mysql 会自动转 为字符串，但是不使用索引。 查看索引使用情况如果索引正在工作， Handler_read_key 的值将很高，这个值代表了一个行被索引值读的次数，很低的值表明增加索引得到的性能改善不高，因为索引并不经常使 用。 Handler_read_rnd_next 的值高则意味着查询运行低效，并且应该建立索引补救。这个值的含义是在数据文件中读下一行的请求数。如果你正进行大量的表扫描， 该值较高。通常说明表索引不正确或写入的查询没有利用索引。 语法： 1show status like 'Handler_read%'; 具体优化查询语句 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，建议使用默认值 1234select id from t where num is null;# 可以在 num 上设置默认值 0，确保表中 num 列没有 null 值select id from t where num=0; 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符 MySQL 只有对以下操作符才使用索引：&lt;，&lt;=，=，&gt;，&gt;=，BETWEEN，IN，以及某些时候的 LIKE。 可以在 LIKE 操作中使用索引的情形是指另一个操作数不是以通配符（%或者_）开头的情形。例如: 123SELECT id FROM t WHERE col LIKE 'Mich%'; # 这个查询将使用索引，SELECT id FROM t WHERE col LIKE '%ike'; #这个查询不会使用索引。 应尽量避免在 where 子句中使用 or 来连接条件,所有的 or 条件都必须是独立索引 1234select id from t where num=10 or num=20;# 可以使用 UNION 合并查询：select id from t where num=10 union all select id from t where num=20; in 和 not in 也要慎用 1234select id from t where num in(1,2,3);对于连续的数值，能用 between 就不要用 in 了：Select id from t where num between 1 and 3; 如果在 where 子句中使用参数，也会导致全表扫描。 因为 SQL 只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推 迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： 123select id from t where num=@num;# 可以改为强制查询使用索引select id from t with(index(索引名)) where num=@num; 应尽量避免在 where 子句中对字段进行表达式操作 123select id from t where num/2=100;# 应改为:select id from t where num=100\\*2; 应尽量避免在 where 子句中对字段进行函数操作 123456select id from t where substring(name,1,3)='abc'; --nameselect id from t where datediff(day,createdate,'2005-11-30')=0; --‘2005-11-30’# 应改为:select id from t where name like 'abc%';select id from t where createdate&gt;='2005-11-30' and createdate&lt;'2005-12-1'; 很多时候用 exists 代替 in 是一个好的选择： 1234select num from a where num in(select num from b);用下面的语句替换：select num from a where exists(select 1 from b where num=a.num); 并不是所有索引对查询都有效， SQL 是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL 查询可能不会去利用索引，如一表中有字段 sex，male、female 几乎各一半，那么即使在 sex 上建了索引也对查询效率起不了作用。 索引并不是越多越好， 索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过 6 个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。","link":"/blog/6dd28e9b.html"}],"tags":[{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"学习笔记","slug":"学习笔记","link":"/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"作用域","slug":"作用域","link":"/tags/%E4%BD%9C%E7%94%A8%E5%9F%9F/"},{"name":"闭包","slug":"闭包","link":"/tags/%E9%97%AD%E5%8C%85/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"vim","slug":"vim","link":"/tags/vim/"},{"name":"架构","slug":"架构","link":"/tags/%E6%9E%B6%E6%9E%84/"},{"name":"记录","slug":"记录","link":"/tags/%E8%AE%B0%E5%BD%95/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"数据库设计","slug":"数据库设计","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/"},{"name":"Redis","slug":"Redis","link":"/tags/Redis/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"ES6","slug":"ES6","link":"/tags/ES6/"},{"name":"KOA","slug":"KOA","link":"/tags/KOA/"},{"name":"计算机网络","slug":"计算机网络","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"React","slug":"React","link":"/tags/React/"},{"name":"Express.js","slug":"Express-js","link":"/tags/Express-js/"},{"name":"API","slug":"API","link":"/tags/API/"},{"name":"版本控制","slug":"版本控制","link":"/tags/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"},{"name":"字符编码","slug":"字符编码","link":"/tags/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81/"},{"name":"后端","slug":"后端","link":"/tags/%E5%90%8E%E7%AB%AF/"},{"name":"Rest","slug":"Rest","link":"/tags/Rest/"},{"name":"RPC","slug":"RPC","link":"/tags/RPC/"},{"name":"kubernetes","slug":"kubernetes","link":"/tags/kubernetes/"}],"categories":[{"name":"后端","slug":"后端","link":"/categories/%E5%90%8E%E7%AB%AF/"},{"name":"JavaScript","slug":"JavaScript","link":"/categories/JavaScript/"},{"name":"数据库","slug":"数据库","link":"/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"其他","slug":"其他","link":"/categories/%E5%85%B6%E4%BB%96/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"}]}